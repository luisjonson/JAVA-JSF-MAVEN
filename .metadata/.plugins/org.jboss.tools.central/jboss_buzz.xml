<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0" uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Write Kubernetes in Java with the Java Operator SDK</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/15/write-kubernetes-java-java-operator-sdk" /><author><name>Christophe Laprun</name></author><id>1516637a-e905-4bc1-b6e6-ed887b069697</id><updated>2022-02-15T07:00:00Z</updated><published>2022-02-15T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://javaoperatorsdk.io"&gt;Java Operator SDK&lt;/a&gt;, or JOSDK, is an &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; project that aims to simplify the task of creating &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; Operators using &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt;. The project was started by &lt;a href="https://container-solutions.com"&gt;Container Solutions&lt;/a&gt;, and Red Hat is now a major contributor.&lt;/p&gt; &lt;p&gt;In this article, you will get a brief overview of what Operators are and why it could be interesting to create them in Java. A future article will show you how to create a simple Operator using JOSDK.&lt;/p&gt; &lt;p&gt;As you can guess, this series of articles is principally targeted at Java developers interested in writing Operators in Java. You don't have to be an expert in Operators, Kubernetes, or &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus&lt;/a&gt;. However, a basic understanding of all these topics will help. To learn more, I recommend reading Red Hat Developer's &lt;a href="https://developers.redhat.com/articles/2021/06/11/kubernetes-operators-101-part-1-overview-and-key-features"&gt;Kubernetes Operators 101 series&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Kubernetes Operators: A brief introduction&lt;/h2&gt; &lt;p&gt;Kubernetes has become the &lt;em&gt;de facto&lt;/em&gt; standard platform for deploying cloud applications. At its core, Kubernetes rests on a simple idea: The user communicates the state in which they want a cluster to be, and the platform will strive to realize that goal. A user doesn't need to tell Kubernetes the steps to get there; they just need to specify what that desired end state should look like. Typically, this involves providing the cluster with a materialized version of this desired state in the form of JSON or YAML files, sent to the cluster for consideration using the &lt;a href="https://kubernetes.io/docs/reference/kubectl/overview/"&gt;&lt;code&gt;kubectl&lt;/code&gt;&lt;/a&gt; tool. Assuming the desired state is valid, once it's on the cluster it will be handled by &lt;em&gt;controllers.&lt;/em&gt; Controllers are processes that run on the cluster and monitor the associated resources to reconcile their actual state with the state desired by the user.&lt;/p&gt; &lt;p&gt;Despite this conceptual simplicity, actually operating a Kubernetes cluster is not a trivial undertaking for non-expert users. Notably, deploying and configuring Kubernetes applications typically requires creating several resources, bound together by sometimes complex relations. In particular, developers who might not have experience on the operational side of things often struggle to move their applications from their local development environment to their final cloud destination. Reducing this complexity would therefore reap immense benefits for users, particularly by encapsulating the required operational knowledge in the form of &lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; that could be interacted with at a higher level by users less familiar with the platform. This is what Kubernetes &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/"&gt;Operators&lt;/a&gt; were developed to achieve.&lt;/p&gt; &lt;h3&gt;Custom Resources&lt;/h3&gt; &lt;p&gt;Kubernetes comes with an extension mechanism in the form of &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"&gt;custom resources &lt;/a&gt; (CRs), which allow users to extend the Kubernetes platform in a way similar to how the core platform is implemented. There is not much formal difference between how native and custom resources are handled: both define domain-specific languages (DSLs) controlling one specific aspect of the platform realized by the YAML or JSON representations of the resources. While native resources control aspects that are part of the platform via their associated controllers, custom resources provide another layer on top of these native resources—allowing users to define higher-level abstractions, for example.&lt;/p&gt; &lt;p&gt;However, the platform doesn't know the first thing about these custom resources, so users must first register a controller with the platform to handle them. The combination of a custom resource-defined DSL and an associated controller enables users to define vocabularies that are closer to their business model. They can focus on business-specific aspects of their application rather than worrying about how a specific state will be realized on the cluster; the latter task falls under the responsibility of the associated controller. This pattern makes it possible to encapsulate the operational knowledge implemented by the associated controller behind the DSL provided by the custom resource. That's what Operators are: implementations of this useful pattern.&lt;/p&gt; &lt;p&gt;Operators are therefore quite attractive for those who want to reduce the knowledge required to deploy applications, but they also automate repetitive steps. They offer organizations the possibility of encapsulating business rules or processes behind a declarative "language" expressed by custom resources using a vocabulary tailored to the task at hand instead of dealing with Kubernetes-native resources that are foreign to less technical users. Once an Operator is installed and configured on a cluster, the logic and automation it provides are accessible to cluster users, who only have to deal with the associated DSL.&lt;/p&gt; &lt;h2&gt;Why write Operators in Java?&lt;/h2&gt; &lt;p&gt;Kubernetes and its ecosystem are written in the &lt;a href="https://developers.redhat.com/topics/go"&gt;Go programming language&lt;/a&gt;, and Operators traditionally have been as well. While it's not necessary to write everything in the same language, it's also a reality that the ecosystem is optimized for Go developers. To be fair, Go is well suited for this task: the language is relatively easy to learn and offers good runtime characteristics both in terms of memory and CPU usage. Moreover, several Go projects aim to make the Operator writing process easy:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://sdk.operatorframework.io/"&gt;&lt;code&gt;operator-sdk&lt;/code&gt;&lt;/a&gt; and its command line tool help developers get started faster&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/kubernetes/client-go/"&gt;&lt;code&gt;client-go&lt;/code&gt;&lt;/a&gt; facilitates programmatic interactions with the Kubernetes API server&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/kubernetes/apimachinery"&gt;&lt;code&gt;apimachinery&lt;/code&gt;&lt;/a&gt; and &lt;a href="https://http://github.com/kubernetes/controller-runtime"&gt;&lt;code&gt;controller-runtime&lt;/code&gt;&lt;/a&gt; offer useful utilities and patterns&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;If Go is so good for writing Operators, why would anyone want to do it in Java? For one thing, Java is the language in which a significant number of enterprise applications are written. These applications are traditionally very complex by nature, and companies that rely on them would benefit from simplified ways to deploy and operate them at scale on Kubernetes clusters.&lt;/p&gt; &lt;p&gt;Moreover, the &lt;a href="https://developers.redhat.com/topics/devops"&gt;DevOps&lt;/a&gt; philosophy mandates that developers should also be responsible for deployment to production, maintenance, and other operational aspects of their application's lifecycle. From that perspective, being able to use the same language during all stages of the lifecycle is an attractive proposition.&lt;/p&gt; &lt;p&gt;Finally, Java-focused companies looking to write Kubernetes Operators want to capitalize on the existing wealth of Java experience among their developers. If developers can ramp up quickly in a programming language they already know rather than investing time and energy learning a new one, that offers a non-negligible advantage.&lt;/p&gt; &lt;h2&gt;Java in the cloud?&lt;/h2&gt; &lt;p&gt;That said, if writing Operators in Java offers so many benefits, why aren't more companies doing it? The first reason that comes to mind is that, compared to Go, Java has traditionally been pretty weak when it comes to deploying to the cloud. Indeed, Java is a platform that has been honed over decades for performance on long-running servers. In that context, memory usage or slow startup times are usually not an issue. This particular drawback has been progressively addressed over time, but the fact remains that a typical Java application will use more memory and start more slowly than a Go application. This matters quite a bit in a cloud environment, in which the pod where your application is running can be killed at any time (the &lt;a href="https://www.redhat.com/en/blog/container-tidbits-does-pets-vs-cattle-analogy-still-apply"&gt;cattle versus pet approach&lt;/a&gt;), and where you might need to scale up quickly (in &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless&lt;/a&gt; environments in particular). Memory consumption also affects deployment density: the more memory your application consumes, the more difficult it is to deploy several instances of it on the same cluster where resources are limited.&lt;/p&gt; &lt;p&gt;Several projects have been initiated to improve Java's suitability for cloud environments, among which is &lt;a href="https://quarkus.io"&gt;Quarkus&lt;/a&gt;, on which this series of articles will focus. The Quarkus project describes itself as "a Kubernetes-native Java stack tailored for OpenJDK HotSpot and GraalVM, crafted from the best of breed Java libraries and standards." By moving much of the processing that is typically done by traditional Java stacks at runtime (e.g., annotation processing, properties file parsing, introspection) to build time, Quarkus improves Java application performance in terms of both consumed memory and startup time. By leveraging the &lt;a href="https://graalvm.org"&gt;GraalVM&lt;/a&gt; project, it also enables easier native compilation of Java applications, making them competitive with Go applications and almost removing runtime characteristics from the equation.&lt;/p&gt; &lt;h2&gt;What about framework support?&lt;/h2&gt; &lt;p&gt;However, as we've already noted, even if we're not taking runtime characteristics into account, Go is an attractive language in which to write Operators, thanks in no small part to the framework ecosystem it offers to support such a task. While there are Java clients that rival the &lt;code&gt;client-go&lt;/code&gt; project to help with interacting with the Kubernetes server, these clients only provide low-level abstractions, while the Go ecosystem provides higher-level frameworks and utilities targeted at Operator developers.&lt;/p&gt; &lt;p&gt;That's where JOSDK comes in, offering a framework comparable to what &lt;code&gt;controller-runtime&lt;/code&gt; offers to Go developers, but tailored for Java developers and using Java idioms. JOSDK aims to ease the task of developing Java Operators by providing a framework that deals with low-level events and implements best practices and patterns, thus allowing developers to focus on their Operator's business logic instead of worrying about the low-level operations required to interact with the Kubernetes API server.&lt;/p&gt; &lt;p&gt;Recognizing that Quarkus is particularly well suited for deploying Java applications, and more specifically Operators, in the cloud, Red Hat has taken JOSDK one step further by integrating it into &lt;a href="https://github.com/quarkiverse/quarkus-operator-sdk"&gt;&lt;code&gt;quarkus-operator-sdk&lt;/code&gt;, a Quarkus extension&lt;/a&gt; that simplifies the Java Operator development task even further by focusing on the development experience aspects. Red Hat has also contributed a plug-in for the &lt;a href="https://sdk.operatorframework.io/docs/cli/operator-sdk/"&gt;&lt;code&gt;operator-sdk&lt;/code&gt; command line tool&lt;/a&gt; to allow quick scaffolding of Java Operator projects using JOSDK and its Quarkus extension.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This concludes the first part of this series exploring writing Operators using JOSDK and Quarkus. You got a sense of the motivation for these projects and saw why it is interesting and useful to write Operators in Java.&lt;/p&gt; &lt;p&gt;In the next part of this series, you'll dive into JOSDK's concepts in greater detail and start implementing a Java Operator of your own using its Quarkus extension and the &lt;code&gt;operator-sdk&lt;/code&gt; command-line tool.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/15/write-kubernetes-java-java-operator-sdk" title="Write Kubernetes in Java with the Java Operator SDK"&gt;Write Kubernetes in Java with the Java Operator SDK&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Christophe Laprun</dc:creator><dc:date>2022-02-15T07:00:00Z</dc:date></entry><entry><title type="html">Getting started with Stork Service Discovery on Quarkus</title><link rel="alternate" href="http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-stork-service-discovery-on-quarkus/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/soa-cloud/quarkus/getting-started-with-stork-service-discovery-on-quarkus/</id><updated>2022-02-14T07:17:50Z</updated><content type="html">In modern microservices architectures, services have dynamically assigned locations. Therefore, it’s essential to integrate Service Discovery as part of the picture. In this article you will learn how to leverage Service Discovery using Smallrye Stork framework on top of a Quarkus reactive application. Service discovery in a nutshell Before we dig into this tutorial, we ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>Serialize Debezium events with Apache Avro and OpenShift Service Registry</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/14/serialize-debezium-events-apache-avro-and-openshift-service-registry" /><author><name>Hugo Guerrero</name></author><id>491a421d-5483-4dde-9f1d-e32456a42240</id><updated>2022-02-14T07:00:00Z</updated><published>2022-02-14T07:00:00Z</published><summary type="html">&lt;p&gt;Change data capture (CDC) is a powerful data processing tool widely used in the industry, and provided by the open source&lt;a href="https://debezium.io/"&gt; Debezium&lt;/a&gt; project. CDC notifies your application whenever changes are made to a data set so that you can react promptly. This tutorial demonstrates how to use Debezium to monitor a MySQL database. As the data in the database changes, the resulting event streams are reflected in &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Debezium includes connectors for many types of data stores. In this tutorial, we will use the &lt;a href="https://debezium.io/documentation/reference/1.3/connectors/mysql.html"&gt;MySQL connector&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Red Hat OpenShift Service Registry&lt;/h2&gt; &lt;p&gt;This demo uses &lt;a href="https://www.redhat.com/es/technologies/cloud-computing/openshift/openshift-service-registry"&gt;Red Hat OpenShift Service Registry&lt;/a&gt;, a fully hosted and managed service that provides an &lt;a href="https://developers.redhat.com/topics/api-management/"&gt;API&lt;/a&gt; and schema registry for &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;. OpenShift Service Registry makes it easy for development teams to publish, discover, and reuse APIs and schemas.&lt;/p&gt; &lt;p&gt;The following services include OpenShift Service Registry at no additional charge:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.redhat.com/es/technologies/cloud-computing/openshift/openshift-api-management"&gt;Red Hat OpenShift API Management&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/es/technologies/cloud-computing/openshift/openshift-streams-for-apache-kafka"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Debezium schema serialization&lt;/h2&gt; &lt;p&gt;Although Debezium makes it easy to capture database changes and record them in &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Kafka&lt;/a&gt;, one of the critical decisions you have to make is &lt;em&gt;how&lt;/em&gt; you will serialize those change events in Kafka. Debezium allows you to select key and value &lt;em&gt;converters&lt;/em&gt; to choose from different types of options. OpenShift Service Registry enables you to store externalized schema versions to minimize the payload that is propagated.&lt;/p&gt; &lt;p&gt;By default, Debezium's converter includes the record's JSON message schema in each record, making the records very verbose. Alternatively, you can serialize the record keys and values using the compact binary format standardized in&lt;a href="https://avro.apache.org/"&gt; Apache Avro&lt;/a&gt;. To use Apache Avro serialization, you must deploy a schema registry that manages Avro message schemas and their versions.&lt;/p&gt; &lt;p&gt;OpenShift Service Registry provides an Avro converter that you can specify in Debezium connector configurations. This converter maps &lt;a href="https://docs.confluent.io/platform/current/connect/index.html"&gt;Kafka Connect&lt;/a&gt; schemas to Avro schemas. The converter then uses the Avro schemas to serialize the record keys and values into Avro's format.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;Install the following tools to run the tasks in this tutorial:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;The latest version of &lt;a href="https://www.docker.com/get-started"&gt;Docker&lt;/a&gt; or &lt;a href="https://podman.io/"&gt;Podman&lt;/a&gt;. (See the&lt;a href="https://docs.docker.com/engine/installation/"&gt; Docker Engine installation documentation&lt;/a&gt; or the &lt;a href="https://podman.io/getting-started/installation"&gt;Podman installation documentation&lt;/a&gt;.) &lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/edenhill/kcat"&gt;&lt;code&gt;kcat&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://github.com/kcctl/kcctl"&gt;&lt;code&gt;kcctl&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The &lt;a href="https://github.com/redhat-developer/app-services-cli/releases/latest"&gt;Red Hat Openshift Application Services CLI, &lt;code&gt;rhoas&lt;/code&gt;&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://stedolan.github.io/jq/"&gt;&lt;code&gt;jq&lt;/code&gt;&lt;/a&gt; (for JSON processing).&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You also need access to the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;A Red Hat Developer account. (As part of the developer program for OpenShift Streams for Apache Kafka, anyone with a Red Hat account can create a Kafka instance free of charge.)&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A running &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;OpenShift Streams cluster&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;A running &lt;a href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry"&gt;OpenShfit Service Registry instance&lt;/a&gt;.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Start the local services&lt;/h2&gt; &lt;p&gt;The MySQL database and the Kafka Connect cluster run locally on your machine for this demo. We will use &lt;a href="https://docs.docker.com/compose/"&gt;Docker Compose&lt;/a&gt; to start the required services, so there is no need to install anything beyond the prerequisites listed in the previous section.&lt;/p&gt; &lt;p&gt;To start the local services, follow these steps.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Clone this repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone https://github.com/hguerrero/debezium-examples.git&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Change to the following directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ cd debezium-examples/debezium-openshift-registry-avro&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open the &lt;code&gt;docker-compose.yaml&lt;/code&gt; file and edit the Kafka-related properties to specify your cluster information. You need to know the name of your Kafka bootstrap server and the service account you will use to connect. The container image will then take the password from a local file called &lt;code&gt;cpass&lt;/code&gt;. Thus, your properties should look like:&lt;/p&gt; &lt;pre&gt; &lt;code&gt; KAFKA_CONNECT_BOOTSTRAP_SERVERS: &lt;your-boostrap-server&gt;:&lt;port&gt; KAFKA_CONNECT_TLS: 'true' KAFKA_CONNECT_SASL_MECHANISM: plain KAFKA_CONNECT_SASL_USERNAME: &lt;kafka-sa-client-id&gt; KAFKA_CONNECT_SASL_PASSWORD_FILE: cpass&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Open the provided &lt;code&gt;cpass&lt;/code&gt; file and replace the placeholder with your service account secret.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Start the environment using one of these two commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;podman-compose up -d&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;docker-compose up -d&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The preceding command starts the following components:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A single-node Kafka Connect cluster&lt;/li&gt; &lt;li&gt;The MySQL database (ready for CDC)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Configure Apicurio converters&lt;/h2&gt; &lt;p&gt;The open source&lt;a href="https://www.apicur.io/registry/"&gt; Apicurio Registry project&lt;/a&gt; is the upstream community that furnishes the technology used by OpenShift Service Registry. Apicurio Registry provides Kafka Connect converters for Apache Avro and JSON Schema. When configuring Avro at the Debezium Connector, you have to specify the converter and schema registry as a part of the connector's configuration. The connector configuration customizes the connector, explicitly setting its serializers and deserializers to use Avro and specifying the location of the Apicurio registry.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/topics/containers/"&gt;container&lt;/a&gt; image used in this environment includes all the required libraries to gain access to the Debezium connectors and Apicurio Registry converters.&lt;/p&gt; &lt;p&gt;The following snippet contains the lines required in the connector configuration to set the &lt;code&gt;key&lt;/code&gt; and &lt;code&gt;value&lt;/code&gt; converters and their respective registry configuration. Replace the placeholders in the snippet with the information from your OpenShift services:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;"key.converter": "io.apicurio.registry.utils.converter.AvroConverter", "key.converter.apicurio.registry.converter.serializer": "io.apicurio.registry.serde.avro.AvroKafkaSerializer", "key.converter.apicurio.registry.url": "&lt;your-service-registry-core-api-url&gt;", "key.converter.apicurio.auth.service.url": "https://identity.api.openshift.com/auth", "key.converter.apicurio.auth.realm": "rhoas", "key.converter.apicurio.auth.client.id": "&lt;registry-sa-client-id&gt;", "key.converter.apicurio.auth.client.secret": "&lt;registry-sa-client-id&gt;", "key.converter.apicurio.registry.as-confluent": "true", "key.converter.apicurio.registry.auto-register": "true", "value.converter": "io.apicurio.registry.utils.converter.AvroConverter", "value.converter.apicurio.registry.converter.serializer": "io.apicurio.registry.serde.avro.AvroKafkaSerializer", "value.converter.apicurio.registry.url": "&lt;your-service-registry-core-api-url&gt;", "value.converter.apicurio.auth.service.url": "https://identity.api.openshift.com/auth", "value.converter.apicurio.auth.realm": "rhoas", "value.converter.apicurio.auth.client.id": "&lt;registry-sa-client-id&gt;", "value.converter.apicurio.auth.client.secret": "&lt;registry-sa-client-id&gt;", "value.converter.apicurio.registry.as-confluent": "true", "value.converter.apicurio.registry.auto-register": "true"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The compatibility mode allows you to use other providers' tooling to deserialize and reuse the schemas in the Apicurio service registry.&lt;/p&gt; &lt;p&gt;This configuration also includes the information required for the serializer to authenticate with the service registry using a service account.&lt;/p&gt; &lt;h2&gt;Create the topics in OpenShift Streams for Apache Kafka&lt;/h2&gt; &lt;p&gt;You need to manually create the required topics Debezium will use in your Kafka cluster. The user interface (UI) in OpenShift Streams for Apache Kafka makes configuration easier and eliminates some sources of errors. The recommended parameters for each topic are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Partitions: &lt;code&gt;1&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Retention time: &lt;code&gt;604800000 ms (7 days)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;Retention size: &lt;code&gt;Unlimited&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;&lt;strong&gt;T&lt;/strong&gt;he topics whose names start with &lt;code&gt;debezium-cluster-&lt;/code&gt; must also be configured with the &lt;code&gt;compact&lt;/code&gt; policy (Figure 1). If you don't set this property correctly, the connector won't be able to start and errors will appear in the Kafka Connect log.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/cleanup-policy-debezium.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/cleanup-policy-debezium.png?itok=-1Hzd0_k" width="1440" height="763" alt="The cleanup policy must be "compact."" loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. The cleanup policy must be "compact." &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The topics to create are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;avro&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.addresses&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.customers&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.geom&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.orders)&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.products&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;avro.inventory.products_on_hand&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;debezium-cluster-configs&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;debezium-cluster-offsets&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;debezium-cluster-status&lt;/code&gt;&lt;/li&gt; &lt;li&gt;&lt;code&gt;schema-changes.inventory&lt;/code&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The resulting table of topics should look like Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/topics-openshift-streams-debezium.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/topics-openshift-streams-debezium.png?itok=WhHlQzfy" width="1440" height="667" alt="When configured correctly, all the topics show up in the user interface." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. When configured correctly, all the topics show up in the user interface. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Configure the database history&lt;/h2&gt; &lt;p&gt;In a separate &lt;a href="https://debezium.io/documentation/reference/stable/connectors/mysql.html#mysql-schema-history-topic"&gt;database history Kafka topic&lt;/a&gt;, the Debezium connector for MySQL records all data definition language (DDL) statements along with the position in the binlog where each DDL statement appears. In order to store that information, the connector needs access to the target Kafka cluster, so you need to add the connection details to the connector configuration.&lt;/p&gt; &lt;p&gt;Create the following lines, inserting the correct values for your environment in your connector configuration to access OpenShift Streams for Apache Kafka. Add the lines as you did with the details of the converter. You need to configure the producer and consumer authentication independently:&lt;/p&gt; &lt;pre&gt; &lt;code class="java"&gt;"database.history.kafka.topic": "schema-changes.inventory", "database.history.kafka.bootstrap.servers": "&lt;your-boostrap-server&gt;", "database.history.producer.security.protocol": "SASL_SSL", "database.history.producer.sasl.mechanism": "PLAIN", "database.history.producer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=&lt;kafka-sa-client-id&gt; password=&lt;kafka-sa-client-secret&gt;;", "database.history.consumer.security.protocol": "SASL_SSL", "database.history.consumer.sasl.mechanism": "PLAIN", "database.history.consumer.sasl.jaas.config": "org.apache.kafka.common.security.plain.PlainLoginModule required username=&lt;kafka-sa-client-id&gt; password=&lt;kafka-sa-client-secret&gt;;",&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can check the final file configuration by viewing the &lt;code&gt;dbz-mysql-openshift-registry-avro.json&lt;/code&gt; file under the main folder.&lt;/p&gt; &lt;h2&gt;Create the connector&lt;/h2&gt; &lt;p&gt;Now that the configuration for the connector is ready, add the configuration to the Kafka Connect cluster so that it starts the task that captures changes to the database. Use the &lt;code&gt;kcctl&lt;/code&gt; command-line client for Kafka Connect, which allows you to register and examine connectors, delete them, and restart them, among other features.&lt;/p&gt; &lt;p&gt;Configure the &lt;code&gt;kcctl&lt;/code&gt; context:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcctl config set-context --cluster http://localhost:8083 local&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Register the connector:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcctl apply -f dbz-mysql-openshift-registry-avro.json&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Check the registry&lt;/h2&gt; &lt;p&gt;Go to the OpenShift Service Registry console. There you should find all the schema artifacts, as shown in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/registry-debezium-artifacts.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/registry-debezium-artifacts.png?itok=5X4IlJI9" width="1145" height="705" alt="The OpenShift Service Registry console shows all the artifacts created for Debezium." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. The OpenShift Service Registry console shows all the artifacts created for Debezium. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Check the data&lt;/h2&gt; &lt;p&gt;Now use the &lt;code&gt;kcat&lt;/code&gt; command-line utility to query the information stored in the OpenShift Streams Kafka cluster:&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;Set the environment variables in your terminal session to specify your cluster information:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export BOOTSTRAP_SERVER=&lt;replace-with-bootstrap-server&gt; $ export CLIENT_ID=&lt;replace-with-kafka-sa-client-id&gt; $ export CLIENT_SECRET=&lt;replace-with-kafka-sa-client-secret&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Check connectivity by querying the cluster metadata:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcat -b $BOOTSTRAP_SERVER \ -X sasl.mechanisms=PLAIN \ -X security.protocol=SASL_SSL \ -X sasl.username="$CLIENT_ID" \ -X sasl.password="$CLIENT_SECRET" -L&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should get an output similar to the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Metadata for all topics (from broker -1: sasl_ssl://kafkaesque-c-isn-bhfjlsl-g-dana.bf2.kafka.rhcloud.com:443/bootstrap): 3 brokers: broker 0 at broker-0-kafkaesque-c-isn-bhfjlsl-g-dana.bf2.kafka.rhcloud.com:443 (controller) broker 2 at broker-2-kafkaesque-c-isn-bhfjlsl-g-dana.bf2.kafka.rhcloud.com:443 broker 1 at broker-1-kafkaesque-c-isn-bhfjlsl-g-dana.bf2.kafka.rhcloud.com:443 11 topics: topic "avro.inventory.orders" with 1 partitions: partition 0, leader 0, replicas: 0,1,2, isrs: 0,1,2 topic "avro.inventory.addresses" with 1 partitions: partition 0, leader 2, replicas: 2,0,1, isrs: 2,0,1 topic "debezium-cluster-configs" with 1 partitions: partition 0, leader 2, replicas: 2,0,1, isrs: 2,0,1 topic "debezium-cluster-offsets" with 1 partitions: partition 0, leader 0, replicas: 0,1,2, isrs: 0,1,2 topic "avro.inventory.products" with 1 partitions: partition 0, leader 0, replicas: 0,2,1, isrs: 0,2,1 topic "debezium-cluster-status" with 1 partitions: partition 0, leader 0, replicas: 0,2,1, isrs: 0,2,1 topic "avro" with 1 partitions: partition 0, leader 1, replicas: 1,2,0, isrs: 1,2,0 topic "avro.inventory.geom" with 1 partitions: partition 0, leader 0, replicas: 0,2,1, isrs: 0,2,1 topic "schema-changes.inventory" with 1 partitions: partition 0, leader 2, replicas: 2,0,1, isrs: 2,0,1 topic "avro.inventory.products_on_hand" with 1 partitions: partition 0, leader 0, replicas: 0,2,1, isrs: 0,2,1 topic "avro.inventory.customers" with 1 partitions: partition 0, leader 1, replicas: 1,2,0, isrs: 1,2,0&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Now check the records in the &lt;code&gt;customers&lt;/code&gt; topic:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcat -b $BOOTSTRAP_SERVER \ -X sasl.mechanisms=PLAIN \ -X security.protocol=SASL_SSL \ -X sasl.username="$CLIENT_ID" \ -X sasl.password="$CLIENT_SECRET" \ -t avro.inventory.customers -C -e&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should see the following four scrambled records in the terminal:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;� Sally Thomas*sally.thomas@acme.com01.5.4.Final-redhat-00001 mysqavro����trueinventorycustomers mysql-bin.000003�r����_ � George Bailey$gbailey@foobar.com01.5.4.Final-redhat-00001 mysqavro����trueinventorycustomers mysql-bin.000003�r����_ � Edward Walkered@walker.com01.5.4.Final-redhat-00001 mysqavro����trueinventorycustomers mysql-bin.000003�r����_ AnneKretchmar$annek@noanswer.org01.5.4.Final-redhat-00001 mysqavro����lastinventorycustomers mysql-bin.000003�r����_ % Reached end of topic avro.inventory.customers [0] at offset 4&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The garbled formatting shows up because we are using Avro for serialization. The &lt;code&gt;kcat&lt;/code&gt; utility expects text strings and hence cannot convert the format correctly. The following step fixes the problem.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Ask &lt;code&gt;kcat&lt;/code&gt; to connect with the OpenShift Service Registry so it can query the schema used and correctly deserialize the Avro records. OpenShift Service Registry supports various types of authentication; the following command uses basic authentication, specifying credentials in the format &lt;code&gt;https://&lt;username&gt;:&lt;password&gt;@&lt;URL&gt;&lt;/code&gt;.:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kcat -b $BOOTSTRAP_SERVER \ -X sasl.mechanisms=PLAIN \ -X security.protocol=SASL_SSL \ -X sasl.username="$CLIENT_ID" \ -X sasl.password="$CLIENT_SECRET" \ -t avro.inventory.customers -C -e \ -s avro -r https://&lt;registry-sa-client-id&gt;:&lt;registry-sa-client-secret&gt;@&lt;registry-compatibility-api-url&gt; | jq&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now the records are displayed in a nicely formatted JSON structure:&lt;/p&gt; &lt;pre&gt; &lt;code class="javascript"&gt;... { "before": null, "after": { "Value": { "id": 1004, "first_name": "Anne", "last_name": "Kretchmar", "email": "annek@noanswer.org" } }, "source": { "version": "1.5.4.Final-redhat-00001", "connector": "mysql", "name": "avro", "ts_ms": 1642452727355, "snapshot": { "string": "last" }, "db": "inventory", "sequence": null, "table": { "string": "customers" }, "server_id": 0, "gtid": null, "file": "mysql-bin.000003", "pos": 154, "row": 0, "thread": null, "query": null }, "op": "r", "ts_ms": { "long": 1642452727355 }, "transaction": null }&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Congratulations! You can send Avro serialized records from MySQL to OpenShift Streams for Apache Kafka using OpenShift Service Registry. Visit the following links to learn more:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka"&gt;Get started with Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry"&gt;Getting started with OpenShfit Service Registry&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/videos/youtube/QYbXDp4Vu-8"&gt;Apache Kafka and Debezium | DevNation Tech Talk&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/topics/event-driven/connectors"&gt;Debezium Apache Kafka connectors&lt;/a&gt; from &lt;a href="https://developers.redhat.com/integration"&gt;Red Hat Integration&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/14/serialize-debezium-events-apache-avro-and-openshift-service-registry" title="Serialize Debezium events with Apache Avro and OpenShift Service Registry"&gt;Serialize Debezium events with Apache Avro and OpenShift Service Registry&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Hugo Guerrero</dc:creator><dc:date>2022-02-14T07:00:00Z</dc:date></entry><entry><title type="html">DevOpsDays Raleigh 2022 - Talking Architecture and Career</title><link rel="alternate" href="http://www.schabell.org/2022/02/devopsdays-raleigh-2022-architecture-and-career.html" /><author><name>Eric D. Schabell</name></author><id>http://www.schabell.org/2022/02/devopsdays-raleigh-2022-architecture-and-career.html</id><updated>2022-02-14T06:00:00Z</updated><content type="html"> It's been some years since , mostly due to the worldly situation and travel restrictions, but thought this year was a great time to jump back in. The 2022 edition will be held on April 13-14 in Raleigh, NC. You can and join the fun for a really interesting array of talks and workshops.  I'm going to give it a good shot this year and have submitted two sessions and a workshop.  The first session is a simple and yet motivational talk based on a theme that I've used before in keynotes to open several other conferences. It's relevant to all parts of our industry so I've pushed it here with the focus on DevOps. OPEN IS KEY TO YOUR DEVOPS CAREER It's not coincidence. It's not luck. It's not going to happen by itself, so what's the secret sauce? Understanding what makes a DevOps career in open source grow, what choices are crucial, and what actions accelerate or damage your future are sometimes hard to grasp. Learning to position, expand and grow your personal DevOps brand in the open source world is what this session provides. Be ready for your next step in open source. Join me for a story sharing a clear and easy to use plan for jump starting your DevOps open source career immediately. The next session is more from my series called Talking Architecture Shop. This again will focus on architecture research for solutions in the DevOps domain that scale.  TALKING ARCHITECTURE SHOP - EXPLORING OPEN SOURCE DEVOPS AT SCALE  You've heard of large scale open source architectures, but have you ever wanted to take a serious look at these real life enterprise DevOps implementations that scale? This session takes attendees on a tour of multiple use cases covering DevOps challenges with hybrid cloud management with GitOps, DevOps in healthcare, and much more. Not only are these architectures interesting, but they are successful real life implementations featuring open source technologies and power many of your own online experiences. The attendee departs this session with a working knowledge of how to map general open source technologies to their solutions. Material covered is available freely online and attendees can use these solutions as starting points for aligning to their own solution architectures. Join us for an hour of power as we talk architecture shop!  Finally, nothing beats a hands-on workshop, so I shared the DevOps Heroes workshop that I presented back in 2019, but updated for this go around. CREATING REAL DEVOPS HEROES - ADDING PROCESS AUTOMATION TO TOOLBOX DevOps is more than the process of automating your CI/CD pipelines to generate code and deployment artefacts for production. It's also about organizational change and integration of many subtle processes that help you to deliver applications seamlessly from development to production through your operations. Let's unlock the power of process integration with a hands-on workshop using your own devices (laptops). We'll take you through the integration of an organizational process as part of your DevOps strategy. Step-by-step you'll build a domain model, creating an automated process, integrating user approval tasks and more using modern open source process automation tooling.  Bring your laptop as this is a hands on experience that takes you from nothing to a fully working DevOps supporting automation integration project. No experience in automation integration is required. Let's add a new tool to your development toolbox and get you jump started on automation integration that's supporting your organizations DevOps activities. Fingers crossed that the selection committee likes what they see and we get invited to meet with you face to face. It's about time, don't you think?</content><dc:creator>Eric D. Schabell</dc:creator></entry><entry><title type="html">Keycloak 17.0.0 released</title><link rel="alternate" href="https://www.keycloak.org/2022/02/keycloak-1700-released" /><author><name /></author><id>https://www.keycloak.org/2022/02/keycloak-1700-released</id><updated>2022-02-11T00:00:00Z</updated><content type="html">To download the release go to . RELEASE NOTES HIGHLIGHTS QUARKUS DISTRIBUTION IS NOW FULLY SUPPORTED The default Keycloak distribution is now based on Quarkus. The new distribution is faster, leaner, and a lot easier to configure! We appreciate migrating from the WildFly distribution is not going to be straightforward for everyone, since how you start and configure Keycloak has radically changed. With that in mind we will continue to support the WildFly distribution until June 2022. For information on how to migrate to the new distribution check out the . QUARKUS DISTRIBUTION UPDATES A lot of effort went into polishing and improving the Quarkus distribution to make it as good as an experience as possible. A few highlights include: * A new approach to documentation in form of server guides to help you install and configure Keycloak * Upgraded Quarkus to 2.7.0.Final * Configuration file is on longer Java specific, and aligns configuration keys with CLI arguments * Clearer separation between build options and runtime configuration. * h2-mem and h2-file databases renamed to dev-mem and dev-file. * Simplified enabling and disabling features * Custom, and unsupported, Quarkus configuration is done through conf/quarkus.properties. * Ability to add custom Java Options via JAVA_OPTS_APPEND (thanks to ) * Initial logging capabilities * Initial support for Cross-DC * User-defined profiles are no longer supported but using different configuration files to achieve the same goal * Quickstarts updated to use the new distribution == Other improvements OFFLINE SESSIONS LAZY LOADED The offline sessions are now lazily fetched from the database by default instead of preloading during the server startup. To change the default behavior, see . IMPROVED USER SEARCH Keycloak now supports a glob-like syntax for the user search when listing users in the Admin Console, which allows for three different types of searches: prefix (foo* which became the default search), infix (*foo*), and exact "foo") MIGRATION FROM 16.1 Before you upgrade remember to backup your database. If you are not on the previous release refer to for a complete list of migration changes. DEFAULT DISTRIBUTION IS NOW POWERED BY QUARKUS The default distribution of Keycloak is now powered by Quarkus, which brings a number of breaking changes to you configure Keycloak and deploy custom providers. For more information check out the . The WildFly distribution of Keycloak is now deprecated, with support ending June 2022. We recommend migrating to the Quarkus distribution as soon as possible. However, if you need to remain on the legacy WildFly distribution for some time, there are some changes to consider: * Container images for the legacy distribution tags have changed. To use the legacy distribution use the tags legacy or 17.0.0-legacy. * Download on the website for the legacy distribution has changed to keycloak-legacy-17.0.0.[zip|tar.gz]. If you encounter problems migrating to the Quarkus distribution, missing ability to configure something, or have general ideas and feedback, please open a discussion in . MIGRATING FROM THE PREVIEW QUARKUS DISTRIBUTION A number of things have changed since the preview Quarkus distribution was released in Keycloak 15.1.0. The ideal way to learn about what’s changed is to check out the new . In summary, the changes include: * Container now published to quay.io/keycloak/keycloak:latest and quay.io/keycloak/keycloak:17.0.0 * Download on website renamed to keycloak-17.0.0.[zip|tar.gz]. * conf/keycloak.properties changed to conf/keycloak.conf, which unifies configuration keys between the config file and CLI arguments. * Clearer separation between build options and runtime configuration. * Custom Quarkus configuration is done through conf/quarkus.properties. * h2-mem and h2-file databases renamed to dev-mem and dev-file. * Features are now enabled/disabled with --features and --features-disabled replacing the previous approach that had an separate config key for each feature. * Runtime configuration can no longer be passed to kc.[sh|bat] build and is no longer persisted in the build * Logging level and format is now configured with --log-level and --log-format, while in the past these had to be configured using unsupported Quarkus properties. CLIENT POLICIES MIGRATION : CLIENT-SCOPES If you used a policy including client-scopes condition and edited JSON document directly, you will need to change the "scope" field name in a JSON document to "scopes". LIQUIBASE UPGRADED TO VERSION 4.6.2 Liquibase was updated from version 3.5.5 to 4.6.2, which includes, among other things, several bug fixes, and a new way of registering custom extensions using ServiceLoader. Migration from previous Keycloak versions to Keycloak 17.0.0 has been extensively tested with all currently supported databases, but we would like to stress the importance of closely following the , specifically of backing up existing database before upgrade. While we did our best to test the consequences of the Liquibase upgrade, some installations could be using specific setup unknown to us. ALL RESOLVED ISSUES NEW FEATURES * Convert MapUserEntity to interface keycloak storage * Create Operator.X module in repo keycloak operator * Keycloak.X deployment keycloak operator * Realm CRD keycloak operator * Testsuite baseline keycloak operator * Let users configure Dynamic Client Scopes keycloak * Create an internal representation of RAR that also handles Static and Dynamic Client Scopes keycloak * Handle Dynamic Scopes correctly in the consent screen keycloak * Publish ECMAScript Modules for keycloak-js keycloak adapter/javascript * Package server guides to be used in the website keycloak dist/quarkus * JPA map storage: Client scope no-downtime store keycloak storage * Convert authorization services entities into interface keycloak storage * Generate the CRD from RealmRepresentation keycloak operator * Improve user search query keycloak storage * Configurable session limits keycloak authentication ENHANCEMENTS * Update Kubernetes and OpenShift examples used by getting started guides to use Quarkus dist keycloak-quickstarts * Update getting-started in QuickStarts to use Quarkus dist keycloak-quickstarts * Update default container to Quarkus keycloak-containers * Update legacy image coordinates in the image manager keycloak-operator * Update documentation for Quarkus distribution keycloak-documentation * Release notes for Keycloak 17 keycloak-documentation * Migration from Keycloak.X preview keycloak-documentation * Documentation for Quarkus distribution keycloak dist/quarkus * Disable pre-loading offline sessions by default keycloak dist/quarkus * Remove Hashicorp Support keycloak dist/quarkus * Make JsonbType generic keycloak storage * Tree storage: introduce notion of per-field primary and cached status in an entity keycloak storage * Provide documentation for proxy mode in Quarkus based Keycloak keycloak dist/quarkus * Review README files. keycloak dist/quarkus * Add indexing to HotRodGroupEntity keycloak storage * Make JpaClientStorage* classes generic keycloak storage * Refactor generated constructors in new store entities keycloak storage * Avoid building configuration all the time when running tests keycloak * Remove override on xmlsec in quarkus/pom.xml keycloak dist/quarkus * Database configuration tests keycloak * Upgrade Infinispan to 12.1.7.Final keycloak storage * Cross-site validation for lazy loading of offline sessions keycloak storage * Switch default offline sessions to lazy loaded keycloak docs * HotRod map storage uses regex pattern that could be precompiled keycloak storage * Add configuration guide keycloak dist/quarkus * Validation for CIBA binding_message parameter keycloak * Upgrade Liquibase to 4.6.2 keycloak storage * Add more details about 2FA to authenticate page keycloak * Verify the WebAuthn functionality and settings for authentication keycloak testsuite * Enable only TLSv1.3 as default for the https protocol and set expected values keycloak dist/quarkus * Backward compatibility for lower-case bearer type in token responses keycloak * Test scenarios for verifying of JS injection for WebAuthn Policy keycloak testsuite * Improve the kustomize setup for the operator keycloak operator * Readiness and Liveness probe for the operator deployment of Keycloak.X keycloak operator * Multiple warnings caused by typed varargs in TokenVerifier keycloak * optimize title/summary and headlines for proxy guide keycloak dist/quarkus * Add support to linking between guides keycloak docs * Remove output of summary in guides keycloak docs * Build command should only accept built-time options keycloak dist/quarkus * Exclude some folders from our SAST analysis keycloak * Convert MapClientScopeEntity to interface keycloak storage * Add a quarkus.properties for unsupported configuration options keycloak dist/quarkus * Remove any reference to configuration profile keycloak dist/quarkus * Remove system property from help message keycloak dist/quarkus * Hide Hasicorp Vault from CLI keycloak dist/quarkus * Improve enabling/disabling features in Quarkus distribution keycloak dist/quarkus * Device Authorization Grant with PKCE keycloak * Adpaters for Map Storage swallow multi-valued attribute while Keycloak Core doesn't support them keycloak storage * Restrict Dynamic Scopes to optional Client Scopes keycloak * Add section recommended exposed paths to reverse proxy documentation keycloak docs * Combine package files for JS adapter keycloak adapter/javascript * Add test scenarios for Passwordless Webauthn AIA keycloak testsuite * Extend and fix tests for Resident Keys for WebAuthn keycloak testsuite * Store information about transport media of WebAuthn authenticator keycloak authentication/webauthn * Sort options in guides by key keycloak docs * Update default ZIP distribution to Quarkus keycloak dist/wildfly * Complete support for Passwordless tests keycloak testsuite * Use keycloak.v2 admin theme by default if admin2 is enabled keycloak admin/ui * Quarkus update to 2.7.0 Final keycloak dist/quarkus * Initial logging support keycloak dist/quarkus * Add support for pinning guides to the top keycloak docs * Implement the Dynamic Scopes parsing for the resource-owner-password-credentials grant. keycloak oidc * Verify if enabling authentication and encryption for JGroups work on Quarkus dist keycloak dist/quarkus * Add note about escaping of vaules for config keycloak dist/quarkus * Logging guide for Quarkus dist keycloak dist/quarkus * Update com.github.ua-parser:uap-java to 1.5.2 keycloak dependencies * Remove external Collection utility class for WebAuthn keycloak authentication/webauthn * Cover enabling mtls in TLS guide keycloak dist/quarkus * Updated use of generics in JPA Map Storage keycloak storage * Create common parent for Jpa*AttributeEntity keycloak storage * Reduce Keycloak.x image size keycloak dist/quarkus BUGS * Incorrect dependency in package.json keycloak-nodejs-connect * KeycloakAuthenticatorValve (Tomcat) does not implement createRequestAuthenticator() keycloak adapter/jee * Spurious logs are spilling in Quarkus Distribution.X integration tests keycloak dist/quarkus * The title of the login screen is not translated into Japanese keycloak * Quarkus relational database setup documentation error keycloak * "look-ahead window" of TOTP should be "look around window" keycloak * Expected Scopes of ClientScopesCondition created on Admin UI are not saved onto ClientScopesCondition.Configuration keycloak authorization-services * Password credential decoding from DB may fail in rare cases - No login possible keycloak * Dist.X cannot connect to external Postgres if the password ends with an = sign keycloak * Dist.X apparently doesn't apply correctly the db schema keycloak * JPA-Map storage might loose writes due to missing locking mechanism keycloak storage * Multiple active tabs when realm name equals name of tab in Admin console keycloak admin/ui * Missing german translation for webauthn-doAuthenticate keycloak translations * Hard coded message within account console v2 keycloak account/ui * Client Policies : Condition's negative logic configuration is not shown in Admin Console's form view keycloak * Placeholders in keycloak.properties do not get substituted at runtime after a build keycloak * Keycloak Server throws NPE at startup when the MAP_STORAGE feature is enabled keycloak storage * Setting "24 mins" to timeout, the admin console displays "1 day" keycloak admin/ui * Username editable when user is forced to re-authenticate keycloak authentication * Quarkus dist "providers" dir has outdated README keycloak dist/quarkus * Newline in localization messages causes uncaught syntax error in account console v2 keycloak account/ui * Dist.X argument parsing fails on semicolon keycloak dist/quarkus * KEYCLOAK-19289 check if values to set is not null keycloak * LDAP connection timeout is treated as login failure and brute force locking the user keycloak * Different method getGroupsCountByNameContaining in MapGroupProvider and JpaRealmProvider keycloak storage * MapRoleProvider could return also client roles when searching for realm roles keycloak storage * Missing DB constraints for JPA Map Storage for Clients keycloak storage * Scope bug in device authorization request keycloak * Handling lazy loading exceptions for storage in new and old storage keycloak storage * Model tests consistently time out keycloak storage * Keycloak.X cannot lookup embedded theme-resources from extension jars keycloak dist/quarkus * WebAuthnSigningInTest failures in pipeline keycloak testsuite * GHA failing due to wrong scheme when downloading ISPN server keycloak * Fixes for token revocation keycloak oidc * JPA Map storage doesn't downgrade entityVersion when modifying a row written with a future entityVersion keycloak storage * Updated flag disappearing for nested entities in HotRod store keycloak storage * Build command exits with success with invalid arguments keycloak dist/quarkus * Review guides to use the correct format for options keycloak docs * Mapped Quarkus properties should not be persisted keycloak dist/quarkus * Unstable model tests when starting multiple Infinispan instances keycloak storage * JPA Map storage doesn't increment version column on attribute update keycloak storage * Update Portuguese (Brazil) translations keycloak translations * Do not run re-augmentation if config is the same in dev mode keycloak dist/quarkus * Errors from CLI are masked by help keycloak dist/quarkus * Rename h2-file/h2-mem to dev-file/dev-mem and remove default values for username/password keycloak dist/quarkus * Keycloak is not capturing proper Signing details(like browser name and version) when logged in from different browsers keycloak account/ui * Not possible to register webauthn key on Firefox keycloak authentication/webauthn * JPA Map storage listener should handle optimistic locking for deleting entities keycloak storage * Failing to use cache remote-stores due to missing dependencies keycloak dist/quarkus * Can not set a jgroups stack other than the defaults from Infinispan keycloak dist/quarkus * JPA delegates can throw NoResultException when entity doesn't have any attributes keycloak storage UPGRADING Before you upgrade remember to backup your database and check the for anything that may have changed.</content><dc:creator /></entry><entry><title type="html">JSON DataSets in Dashbuilder</title><link rel="alternate" href="https://blog.kie.org/2022/02/json-datasets-in-dashbuilder.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2022/02/json-datasets-in-dashbuilder.html</id><updated>2022-02-10T16:47:16Z</updated><content type="html">Datasets providers run on the same process as Dashbuilder, which means that if you only use a specific provider, all the others are idle, as part of Dashbuilder. In addition to that, we need to maintain all providers.  In the most recent release a new dataset provider type was added: External Data Sets. With External DataSets any JSON array can be a dataset. Simplest dataset Dashbuilder tries to guess the column type (LABEL, NUMBER, DATE, default type is label) and gives it a generic name. Users that want to force a type can use a more complex JSON to include column information: Sample dataset with columns information The source for the JSON can be an HTTP URL or a file URL, meaning that any JSON on the WEB can be used as a dataset.  The URL is the only information that the user must provide about the dataset and the whole dataset is loaded into memory each time it is accessed. If the source JSON is not changed then users can set up a cache for the dataset.  Bear in mind that only the “Data refresh every” is used by the external provider. The URL for the dataset can be changed at runtime using Java System property dashbuilder.dataset.%s.url – replace %s with the dataset name – to point to a new dataset JSON. Users can add more rows to this new definition or change the existing rows, but do not change columns definition (type and ID) or it will result in an expected error in the reports that use the dataset. FUTURE Client Loading A new feature to call datasets from the client side is in development. It will only support  HTTP URLs and users must ensure that CORS is enabled for the dashbuilder domain. Dynamic Lookup Currently the whole dataset is retrieved from the source. A new feature will allow users to receive the front end request (lookup) to retrieve the filtered dataset. The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>Kafka Monthly Digest: January 2022</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/10/kafka-monthly-digest-january-2022" /><author><name>Mickael Maison</name></author><id>ded30a3f-38d2-4493-ad49-5a070aef3af2</id><updated>2022-02-10T07:00:00Z</updated><published>2022-02-10T07:00:00Z</published><summary type="html">&lt;p&gt;This 48th edition of the Kafka Monthly Digest covers what happened in the &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; community in January 2022. The release of Apache Kafka 3.1.0 is the big news, including updates to Kafka Connect and Kafka Streams. I'll also discuss new KIPs and &lt;a href="https://developers.redhat.com/topics/open-source"&gt;open source&lt;/a&gt; releases in January 2022.&lt;/p&gt; &lt;p&gt;This is the fourth anniversary of this series! If you are interested in the history behind the Kafka Monthly Digest, last year I explained on &lt;a href="https://twitter.com/MickaelMaison/status/1355930398677757955"&gt;Twitter&lt;/a&gt; how I got started.&lt;/p&gt; &lt;p&gt;For last month's digest, see &lt;a href="https://developers.redhat.com/articles/2022/01/11/kafka-monthly-digest-december-2021"&gt;Kafka Monthly Digest: December 2021&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Kafka 3.1.0 release&lt;/h2&gt; &lt;p&gt;David Jacot released Kafka 3.1.0 on January 24 and published an announcement on the &lt;a href="https://blogs.apache.org/kafka/entry/what-s-new-in-apache7"&gt;Apache blog&lt;/a&gt;. As always, you can find the complete list of changes in the &lt;a href="https://www.apache.org/dist/kafka/3.1.0/RELEASE_NOTES.html"&gt;release notes&lt;/a&gt; or the &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+3.1.0"&gt;release plan&lt;/a&gt; on the Kafka wiki.&lt;/p&gt; &lt;p&gt;Kafka 3.1 adds support for Java 17. Note that running Kafka in &lt;a href="https://github.com/apache/kafka/blob/trunk/config/kraft/README.md"&gt;KRaft mode&lt;/a&gt; (without ZooKeeper) is still not ready for production.&lt;/p&gt; &lt;p&gt;This new minor release brings a few interesting features, which I'll highlight in the next sections.&lt;/p&gt; &lt;h4&gt;Kafka brokers and clients&lt;/h4&gt; &lt;p&gt;Updates to the Kafka broker and clients include the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;A production-ready OAuth implementation with OpenID Connect (OIDC) support (&lt;a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186877575"&gt;KIP-768&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Metrics improvements include new metrics for active and fenced brokers (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-748%3A+Add+Broker+Count+Metrics"&gt;KIP-748&lt;/a&gt;) and consistent names for latency metrics (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-773%3A+Differentiate+consistently+metric+latency+measured+in+millis+and+nanos"&gt;KIP-773&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Topic IDs that were introduced in Kafka 2.8 are now used in &lt;code&gt;FetchRequests&lt;/code&gt; (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-516%3A+Topic+Identifiers"&gt;KIP-516&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;h4&gt;Kafka Connect&lt;/h4&gt; &lt;p&gt;The &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/connect/mirror/ReplicationPolicy.html"&gt;ReplicationPolicy&lt;/a&gt; interface allows you to customize the names of MirrorMaker internal topics (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-690%3A+Add+additional+configuration+to+control+MirrorMaker+2+internal+topics+naming+convention"&gt;KIP-690&lt;/a&gt;).&lt;/p&gt; &lt;h4&gt;Kafka Streams&lt;/h4&gt; &lt;p&gt;Updates to Kafka Streams include the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Foreign-key joins can now be performed using custom partitioners (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-775%3A+Custom+partitioners+in+foreign+key+joins"&gt;KIP-775&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;Querying &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/state/ReadOnlySessionStore.html"&gt;ReadOnlySessionStore&lt;/a&gt;, &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/state/ReadOnlyWindowStore.html"&gt;ReadOnlyWindowStore&lt;/a&gt;, and &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/state/ReadOnlyKeyValueStore.html"&gt;ReadOnlyKeyValueStore&lt;/a&gt; with unbounded ranges is now supported (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-763%3A+Range+queries+with+open+endpoints"&gt;KIP-763&lt;/a&gt; and &lt;a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=186876596"&gt;KIP-766&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;The eager rebalance protocol is now deprecated. The new protocol, cooperative rebalancing, has been the default since Kafka 2.2. Eager will be removed in the next major release (&lt;a href="https://issues.apache.org/jira/browse/KAFKA-13439"&gt;KAFKA-13439&lt;/a&gt;).&lt;/li&gt; &lt;li&gt;All uncaught exceptions are now wrapped into &lt;a href="https://kafka.apache.org/31/javadoc/org/apache/kafka/streams/errors/StreamsException.html"&gt;StreamsException&lt;/a&gt; (&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-783%3A+Add+TaskId+field+to+StreamsException"&gt;KIP-783&lt;/a&gt;).&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Kafka Improvement Proposals&lt;/h2&gt; &lt;p&gt;Last month, the community submitted seven &lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals"&gt;KIPs&lt;/a&gt; (KIP-812 to KIP-818). I'll highlight just a few of them:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-813%3A+Shareable+State+Stores"&gt;KIP-813: Shareable state stores&lt;/a&gt;: This KIP proposes a mechanism for reusing existing Kafka Streams state stores in other Streams applications. In scenarios where multiple applications perform similar processing steps, this could be useful to avoid doing the same processing twice.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-814%3A+Static+membership+protocol+should+let+the+leader+skip+assignment"&gt;KIP-814: Static membership protocol should let the leader skip assignment&lt;/a&gt;: This KIP's goal is to improve support for static consumer group membership (introduced in Kafka 2.4). Today, in some cases, consumer groups with static membership can end up without a leader and miss metadata updates.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-817%3A+Fix+inconsistency+in+dynamic+application+log+levels"&gt;KIP-817: Fix inconsistency in dynamic application log levels&lt;/a&gt;: There are two methods for changing Kafka's log at runtime: Java Management Extensions (JMX) and the Admin API. However, there are some inconsistencies in the levels they support. For example, it's not possible to set a logger to &lt;code&gt;OFF&lt;/code&gt; via the Admin API. This KIP aims at clearing up those inconsistencies.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Community releases for Apache Kafka&lt;/h2&gt; &lt;p&gt;This section covers a few notable open source community project releases:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;p&gt;&lt;a href="https://a-great-day-out-with.github.io/kafka.html"&gt;A Great Day Out With... Apache Kafka&lt;/a&gt;: This is a curated list of resources, projects, tools, and products for people getting started with Kafka. The most recent update added the latest key Kafka resources people should be aware of when working with the platform.&lt;/p&gt;&lt;/li&gt; &lt;li&gt;&lt;p&gt;&lt;a href="https://github.com/Shopify/sarama/releases/tag/v1.31.0"&gt;Sarama 1.31&lt;/a&gt;: Sarama is a pure &lt;a href="https://developers.redhat.com/topics/go"&gt;Go language&lt;/a&gt; Kafka client. This new release adds support for &lt;code&gt;IncrementalAlterConfigs&lt;/code&gt; in the Admin client, improves request pipelining for the producer, and as usual fixes a few bugs.&lt;/p&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Kafka blogs and articles&lt;/h2&gt; &lt;p&gt;Here are a few of the most noteworthy Kafka-related blogs and articles published in January 2022:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://www.confluent.io/blog/5-common-pitfalls-when-using-apache-kafka/"&gt;5 common pitfalls when using Apache Kafka&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://medium.com/airwallex-engineering/kafka-streams-iterative-development-and-blue-green-deployment-fae88b26e75e"&gt;Kafka Streams: Iterative development and blue-green deployment&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://dev.to/hpgrahsl/towards-a-mongodb-backed-apache-kafka-streams-state-store-20ik"&gt;Towards a MongoDB-backed Apache Kafka Streams state store&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To learn more about Kafka, visit &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Red Hat Developer's Apache Kafka topic page&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/10/kafka-monthly-digest-january-2022" title="Kafka Monthly Digest: January 2022"&gt;Kafka Monthly Digest: January 2022&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mickael Maison</dc:creator><dc:date>2022-02-10T07:00:00Z</dc:date></entry><entry><title type="html">This Week in JBoss - February 10th 2022</title><link rel="alternate" href="https://www.jboss.org/posts/weekly-2022-02-10.html" /><category term="quarkus" /><category term="java" /><category term="resteasy" /><category term="camel" /><category term="event-driven" /><category term="mta" /><category term="keycloak" /><category term="wildfly" /><category term="vertx" /><category term="kogito" /><category term="idaas" /><author><name>Pedro Silva</name><uri>https://www.jboss.org/people/pedro-silva</uri><email>do-not-reply@jboss.com</email></author><id>https://www.jboss.org/posts/weekly-2022-02-10.html</id><updated>2022-02-10T00:00:00Z</updated><content type="html">&lt;article class="" data-tags="quarkus, java, resteasy, camel, event-driven, mta, keycloak, wildfly, vertx, kogito, idaas"&gt; &lt;h1&gt;This Week in JBoss - February 10th 2022&lt;/h1&gt; &lt;p class="preamble"&gt;&lt;/p&gt;&lt;p&gt;Hello! Welcome to the third JBoss weekly editorial of 2022. Enjoy our pick of the latest news and interesting reads from around the JBoss community.&lt;/p&gt;&lt;p&gt;&lt;/p&gt; &lt;div class="sect1"&gt; &lt;h2 id="_releases_releases_releases"&gt;Releases, releases, releases!&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Here are the releases from the JBoss Community for this edition:&lt;/p&gt; &lt;div class="ulist square"&gt; &lt;ul class="square"&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://quarkus.io/blog/quarkus-2-7-1-final-released/"&gt;Quarkus 2.7.1.Final&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/01/kogito-1-16-0-released.html"&gt;Kogito 1.16.0&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_articles_books_and_tutorials"&gt;Articles, Books, and Tutorials&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;p&gt;Check out some of the recent articles about Java, Kubernetes, Quarkus, Keycloak and more…​&lt;/p&gt; &lt;div class="sect2"&gt; &lt;h3 id="_deploy_your_kie_sandbox_to_openshift"&gt;Deploy your KIE Sandbox to Openshift&lt;/h3&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/01/deploy-your-kie-sandbox-to-openshift.html"&gt;Deploy your KIE Sandbox to Openshift&lt;/a&gt; by Guilherme Caponetto&lt;/p&gt; &lt;p&gt;The Kogito Tooling release 0.16.0 includes three container images to make it easy to deploy the KIE Sandbox to an OpenShift instance.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_kie_sandbox_top_7_key_new_features"&gt;KIE Sandbox: Top 7 key new features&lt;/h3&gt; &lt;p&gt;&lt;a href="https://blog.kie.org/2022/02/kie-sandbox-top-7-key-new-features.html"&gt;KIE Sandbox: Top 7 key new features&lt;/a&gt; by Eder Ignatowicz&lt;/p&gt; &lt;p&gt;In the last months of 2021, the “.NEW environment” (bpmn.new, dmn.new) received a massive update, and now it’s named KIE Sandbox! Dealing with complex models and collaborating with others has just become much easier. In this blog post, let’s go for a walkthrough of the top new features of KIE Sandbox.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_artemis_monitoring_in_openshift"&gt;Artemis monitoring in OpenShift&lt;/h3&gt; &lt;p&gt;&lt;a href="https://blog.ramon-gordillo.dev/2022/02/artemis-monitoring-in-openshift/"&gt;Artemis monitoring in OpenShift&lt;/a&gt; by Ramón Gordillo&lt;/p&gt; &lt;p&gt;It is really simple to monitor the brokers deployed on OpenShift and show the metrics in grafana or configuring alerts based on the metrics. We are using the artemis version included in AMQ 7.9 deployed with the operator.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_deprecation_of_keycloak_adapters"&gt;Deprecation of Keycloak adapters&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.keycloak.org/2022/02/adapter-deprecation"&gt;Deprecation of Keycloak adapters&lt;/a&gt; by Stian Thorgersen&lt;/p&gt; &lt;p&gt;This blog is a notice that the Keycloak project will be deprecating the Keycloak Adapters.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_running_arquillian_tests_from_eclipse"&gt;Running Arquillian Tests from Eclipse&lt;/h3&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jboss-frameworks/arquillian/running-arquillian-tests-from-eclipse/"&gt;Running Arquillian Tests from Eclipse&lt;/a&gt; by F. Marchioni&lt;/p&gt; &lt;p&gt;The blog shows you how to configure the Arquillian Tests on Eclipse.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_reverse_engineer_your_jboss_as_wildfly_configuration_to_cli"&gt;Reverse engineer your JBoss AS-WildFly configuration to CLI&lt;/h3&gt; &lt;p&gt;&lt;a href="http://www.mastertheboss.com/jbossas/jboss-script/reverse-engineer-your-jboss-as-wildfly-configuration-to-cli/"&gt;Reverse engineer your JBoss AS-WildFly configuration to CLI&lt;/a&gt; by F. Marchioni&lt;/p&gt; &lt;p&gt;Exporting your WildFly/JBoss EAP configuration to a CLI script is something you are going to need one day or another. No worries, the project Profile Cloner comes to the rescue!&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_intelligent_data_as_a_service_idaas_example_data_insights"&gt;Intelligent data as a service (iDaaS) - Example data insights&lt;/h3&gt; &lt;p&gt;&lt;a href="https://www.schabell.org/2022/01/idaas-example-data-insights.html"&gt;Intelligent data as a service (iDaaS) - Example data insights&lt;/a&gt; by Eric D. Schabell&lt;/p&gt; &lt;p&gt;Part 5 - Data insights through the iDaaS insights architecture provides a different look at the architecture and gives us insights into how our healthcare solutions are performing.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_whats_new_for_developers_in_java_18"&gt;What’s new for developers in Java 18?&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/27/whats-new-developers-java-18"&gt;What’s new for developers in Java 18?&lt;/a&gt; by Shaaf, Syed&lt;/p&gt; &lt;p&gt;This article highlights some of the features that developers can look for in the upcoming Java 18 release, including the new simple web server module, a more sophisticated way to annotate your Javadocs, and the &lt;code&gt;–finalization=disabled option&lt;/code&gt;, which lets you test how a Java application will behave when finalization is removed in a future release. See the end of the article for where to download Java 18 in early access builds.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_build_a_bootable_jar_for_cloud_ready_microservices"&gt;Build a bootable JAR for cloud-ready microservices&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2022/01/26/build-bootable-jar-cloud-ready-microservices"&gt;Build a bootable JAR for cloud-ready microservices&lt;/a&gt; by Mauro Vocale&lt;/p&gt; &lt;p&gt;This article describes how to create a bootable JAR using Red Hat JBoss Enterprise Application Platform (JBoss EAP) and Jakarta EE and incorporate useful extensions, particularly a PostgreSQL database and MicroProfile capabilities.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_connect_to_an_external_postgresql_database_using_ssltls_for_red_hats_single_sign_on_technology"&gt;Connect to an external PostgreSQL database using SSL/TLS for Red Hat’s single sign-on technology&lt;/h3&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/articles/2021/08/31/connect-external-postgresql-database-using-ssltls-red-hats-single-sign"&gt;Connect to an external PostgreSQL database using SSL/TLS for Red Hat’s single sign-on technology&lt;/a&gt; by Olivier Rivat&lt;/p&gt; &lt;p&gt;This article shows you how to connect securely to applications and data sources using Red Hat’s single sign-on technology.&lt;/p&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_videos"&gt;Videos&lt;/h3&gt; &lt;p&gt;Here’s my pick of this week’s YouTube videos:&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=oeDFWjV43zk"&gt;KIE Drop - Custom Forms&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=7M0Tvlx-GTA"&gt;Quarkus Insights #78: Quarkus Example App Demo&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=ypqllaWE0DA"&gt;Migrate Spring Boot to Quarkus in 5 minutes&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=VeKVUNRyH6k"&gt;Workshop: Red Hat OpenShift Streams for Apache Kafka with Service Registry (Feb 2, 2022)&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect2"&gt; &lt;h3 id="_books"&gt;Books&lt;/h3&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/e-books/kubernetes-native-microservices-quarkus-and-microprofile"&gt;Kubernetes Native Microservices with Quarkus and MicroProfile&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;p&gt;&lt;em&gt;That’s all for today! Please join us again in two weeks for another round of our JBoss editorial!&lt;/em&gt;&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="author"&gt; &lt;pfe-avatar pfe-shape="circle" pfe-pattern="squares" pfe-src="/img/people/pedro-silva.png"&gt;&lt;/pfe-avatar&gt; &lt;span&gt;Pedro Silva&lt;/span&gt; &lt;/div&gt;&lt;/article&gt;</content><dc:creator>Pedro Silva</dc:creator></entry><entry><title>Create a data stream with Amazon Kinesis</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/09/create-data-stream-amazon-kinesis" /><author><name>Bob Reselman</name></author><id>41d5024e-7009-4875-b9f0-99ef602d7ecd</id><updated>2022-02-09T07:00:00Z</updated><published>2022-02-09T07:00:00Z</published><summary type="html">&lt;p&gt;Streaming applications process data such as video, audio, and text as a continuous flow of messages. Working with streams adds a new dimension to application programming. The difference between handling &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;events &lt;/a&gt;and data streaming is like going from drinking water one glass at a time to taking it in from a garden hose. This article shows how to get a stream up and running under &lt;a href="https://aws.amazon.com/kinesis/"&gt;Amazon Kinesis&lt;/a&gt;, a stream management service offered by Amazon Web Services (AWS).&lt;/p&gt; &lt;p&gt;Data streaming is a different way of doing business. Making it all work requires both an understanding of some basic streaming patterns and an awareness of the tools and techniques you can use to get the job done. This article takes you from the initial configuration through running a producer and checking reports on its behavior.&lt;/p&gt; &lt;h2&gt;Understanding streaming patterns&lt;/h2&gt; &lt;p&gt;As the name implies, a stream is a continuous flow of data transmitted at high-speed rates between a source and target. Probably the best example of streaming data is video content from a producer such as C-SPAN. The C-SPAN studio streams bytes of video data that make up a telecast through a streaming manager to a data center on the backend. Then that stream is forwarded onto users' computers or smart TVs. In this scenario, you can think of the video source as the producer and the viewers at home as consumers (Figure 1). Multiple consumers are easy to support, which is great for broadcasting.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/single.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/single.png?itok=U-L3jRNX" width="601" height="191" alt="A data stream goes from a producer through a streaming manager to consumers." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1. A data stream goes from a producer through a streaming manager to consumers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;There are also streaming scenarios in which several producers stream data into a streaming manager, which sends them on to consumers. This pattern is shown in Figure 2.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/multiple.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/multiple.png?itok=lkwaEuWw" width="591" height="211" alt="Data streams from multiple producers can be combined in the streaming manager and sent to consumers." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2. Data streams from multiple producers can be combined in the streaming manager and sent to consumers. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;An example of this many-to-many scenario is where many browsers act as producers on the client side of a web application. Each browser sends a continuous stream of messages back to a stream manager on the server side. Each message might describe a particular mouse event from the user's browser.&lt;/p&gt; &lt;p&gt;Then, on the server side, there might be any number of consumers interested in a user's mouse events. The marketing department might be interested in the ads the user clicks on. The user experience team might want to know how much time a user takes moving the mouse around a web page before executing an action. All that any party needs to do to consume information from the stream of interest is establish a connection to the stream manager.&lt;/p&gt; &lt;p&gt;The important thing to understand about streaming is that it's very different from a request/response interaction that handles one message at a time, which is typical when web surfing or using an HTTP API such as REST. In contrast, streams involve working with an enormous amount of messages flowing continuously in one direction all the time.&lt;/p&gt; &lt;p&gt;A number of services support streaming. Google Cloud has its DataFlow service. The &lt;a href="https://www.redhat.com/en/resources/amq-streams-datasheet"&gt;streaming component&lt;/a&gt; of &lt;a href="https://developers.redhat.com/products/amq/overview"&gt;Red Hat AMQ&lt;/a&gt; is based on &lt;a href="https://developers.redhat.com/topics/kafka-kubernetes"&gt;Apache Kafka&lt;/a&gt; and integrates with &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;. There are other services too. In this article, we're going to look at the Amazon Kinesis service.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; To learn how to implement message streams using OpenShift and Kafka, try this activity in the no-cost Developer Sandbox for Red Hat OpenShift: &lt;a href="https://developers.redhat.com/developer-sandbox/activities/connecting-to-your-managed-kafka-instance"&gt;Connecting to your Managed Kafka instance from the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Amazon Kinesis Data Streams&lt;/h2&gt; &lt;p&gt;Amazon Kinesis is made up of a number of subservices such as &lt;a href="https://aws.amazon.com/kinesis/data-streams/"&gt;Amazon Kinesis Data Streams&lt;/a&gt;, &lt;a href="https://aws.amazon.com/kinesis/data-firehose/"&gt;Amazon Kinesis Data Firehose&lt;/a&gt;, and &lt;a href="https://aws.amazon.com/kinesis/video-streams/?amazon-kinesis-video-streams-resources-blog.sort-by=item.additionalFields.createdDate&amp;amazon-kinesis-video-streams-resources-blog.sort-order=desc"&gt;Amazon Kinesis Video Streams&lt;/a&gt;. In this article, we will use Kinesis Data Streams, which is a general streaming manager.&lt;/p&gt; &lt;p&gt;Working with a Kinesis data stream is a three-step process, discussed in the sections that follow:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Create the stream.&lt;/li&gt; &lt;li&gt;Attach a user or user group to the stream. The user or group must have AWS permissions to use the stream.&lt;/li&gt; &lt;li&gt;Submit stream data as that user (group) to the stream on the backend.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;There is actually a fourth step: In order for stream data to be useful, it needs to be processed by a consumer. However, creating and using a consumer is beyond the scope of this article. In this article, we'll just get data into Kinesis Data Streams.&lt;/p&gt; &lt;h2&gt;Set up the stream on Amazon Kinesis&lt;/h2&gt; &lt;p&gt;There are a few ways to set up a Kinesis stream:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Create the stream directly using the AWS command line.&lt;/li&gt; &lt;li&gt;Run an AWS CloudFormation script.&lt;/li&gt; &lt;li&gt;Use the AWS dashboard.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;In this article, we'll use the dashboard.&lt;/p&gt; &lt;p&gt;Figure 3 illustrates the process for creating the Kinesis stream.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/stream.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/stream.png?itok=btRawS8b" width="1440" height="841" alt="From the AWS Services page, choose Kinesis Data Streams and create a stream." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3. From the AWS Services page, choose Kinesis Data Streams and create a stream. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The steps are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Search the AWS &lt;strong&gt;Services&lt;/strong&gt; main page for the term "Kinesis "and then select the &lt;strong&gt;Kinesis&lt;/strong&gt; service.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;Kinesis Data Streams&lt;/strong&gt; option in the dialog that appears.&lt;/li&gt; &lt;li&gt;Enter the name of the new data stream in the &lt;strong&gt;Data stream name&lt;/strong&gt; box. In this case, we'll name the stream &lt;code&gt;general_stream&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create data stream&lt;/strong&gt; button at the bottom of the &lt;strong&gt;Create data stream&lt;/strong&gt; page.&lt;/li&gt; &lt;li&gt;The newly created stream is displayed on the stream's page, in this case, &lt;strong&gt;Amazon Kinesis→Data streams→&lt;/strong&gt;&lt;strong&gt;general_stream&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Create a user for the stream&lt;/h2&gt; &lt;p&gt;After the stream is created, you'll need to create a user in AWS that has to write access to the stream. There are two ways to grant permission. You can create an AWS group that has write access permission to the stream and then add an existing or new user to the group. Another way is to create a new user and give that user the required permission to write to the stream.&lt;/p&gt; &lt;p&gt;For this article, we'll create a new user specifically dedicated to writing to the stream. Please be advised that we're doing this for demonstration purposes only. At the enterprise level, granting permissions to AWS resources is a very formal process that can vary from company to company. Some companies create user groups with specific permissions and assign users to that group. Other companies assign permissions on a user-by-user basis. In this case, we'll take the single-user approach.&lt;/p&gt; &lt;p&gt;Figure 4 illustrates the process.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/user_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/user_1.png?itok=LHR9v4ka" width="1440" height="675" alt="Create a user and save the access information for later use." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4. Create a user and save the access information for later use. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The steps are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Select the &lt;strong&gt;Identity and Access Management (IAM)&lt;/strong&gt; service from the AWS &lt;strong&gt;Services&lt;/strong&gt; page. In the IAM page, select &lt;strong&gt;Users&lt;/strong&gt;. The Users page will appear.&lt;/li&gt; &lt;li&gt;Click the button labeled &lt;strong&gt;Add users&lt;/strong&gt; on the upper right of the page. The &lt;strong&gt;Add user&lt;/strong&gt; page appears.&lt;/li&gt; &lt;li&gt;Enter the name of the user. In this case, we'll enter the name &lt;code&gt;my_kinesis_user&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Check the &lt;strong&gt;Access key&lt;/strong&gt; checkbox. An access key ID and secret access key are generated for this user. These two credentials are very important. You'll use them to allow write access to the stream from outside of AWS.&lt;/li&gt; &lt;li&gt;Once you fill out the first page in the create user process, you'll be presented with pages for setting up permissions, tags, etc. You can just click through these pages without making any entries. We'll set up permissions later on.&lt;/li&gt; &lt;li&gt;Finally, you have created the user. The &lt;strong&gt;Success&lt;/strong&gt; page displays the access key ID and the secret access key. Also, there's a button labeled &lt;strong&gt;Download .csv&lt;/strong&gt;. Click this button to download the &lt;code&gt;.csv&lt;/code&gt; file that contains the access key ID and the secret access key information to your local machine. You'll need that information when creating programs that write to a Kinesis stream as the user you've just created.&lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Define access to the Amazon Kinesis stream&lt;/h2&gt; &lt;p&gt;Now that you've created the special user, you need to create a policy granting write permission to the stream. After the policy is created, you'll assign it to the user that was created in the previous section.&lt;/p&gt; &lt;p&gt;There are a lot of permissions to choose from. However, if you know exactly the permissions you want to assign to the policy, you can define them in JSON format and apply them directly. This is the approach we'll take. Our policy is defined in the following JSON, which describes all the permissions a user needs to have write access to any Kinesis stream:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;{ "Version": "2012-10-17", "Statement": [ { "Effect": "Allow", "Action": [ "kinesis:SubscribeToShard", "kinesis:ListShards", "kinesis:PutRecords", "kinesis:GetShardIterator", "kinesis:DescribeStream", "kinesis:DescribeStreamSummary", "kinesis:DescribeStreamConsumer", "kinesis:RegisterStreamConsumer", "kinesis:PutRecord" ], "Resource": "*" } ] }&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 5 illustrates how to add the permissions.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/policy.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/policy.png?itok=TwXF53R8" width="1440" height="608" alt="Create a policy by pasting in permissions in JSON format and giving the policy a name." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5. Create a policy by pasting in permissions in JSON format and giving the policy a name. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The steps are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;In the &lt;strong&gt;Identity and Access Management (IAM)&lt;/strong&gt; page, click &lt;strong&gt;Policies&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Policy&lt;/strong&gt; button, shown on the right side of the screen. This displays the permissions dialog.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;JSON&lt;/strong&gt; tab in the permissions dialog. Substitute the JSON shown earlier for the placeholder text that appeared initially. Click through the next set of pages until you get to the &lt;strong&gt;Review policy&lt;/strong&gt; page.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Review policy&lt;/strong&gt; page, enter a name for the policy in the text box. In this case, we use the name &lt;code&gt;general-write-access-to-kinesis&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create policy&lt;/strong&gt; button.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Now that we've created the policy, we need to apply it to the previously created user named &lt;code&gt;my_kinesis_user&lt;/code&gt;. That task is the subject of the next section.&lt;/p&gt; &lt;h2&gt;Enable access to the Amazon Kinesis stream&lt;/h2&gt; &lt;p&gt;At this point we have a special user named &lt;code&gt;my_kinesis_user&lt;/code&gt; and a policy named &lt;code&gt;general-write-access-to-kinesis&lt;/code&gt;. Now we need to attach the policy to the user. Figure 6 illustrates the process.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/attach.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/attach.png?itok=gkPspfEY" width="1440" height="1103" alt="Attach your policy to your user on the Users page of the IAM site." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6. Attach your policy to your user on the Users page of the IAM site. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The steps are as follows:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;In the &lt;strong&gt;Identity and Access Management (IAM)&lt;/strong&gt; page, click &lt;strong&gt;Users&lt;/strong&gt; on the left side to return to the &lt;strong&gt;Users&lt;/strong&gt; page. A list of users will appear. Click on the user named &lt;code&gt;my_kinesis_user&lt;/code&gt;. A summary page for that user will appear.&lt;/li&gt; &lt;li&gt;Under the &lt;strong&gt;Permissions&lt;/strong&gt; tab, click the &lt;strong&gt;Add permissions&lt;/strong&gt; button. The &lt;strong&gt;Add permissions&lt;/strong&gt; page will appear.&lt;/li&gt; &lt;li&gt;Select the button labeled &lt;strong&gt;Attach existing policies directly&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;In the &lt;strong&gt;Filter policies&lt;/strong&gt; text box, enter the term, &lt;code&gt;general&lt;/code&gt;. As you type the letters, the custom policy &lt;code&gt;general-write-access-to-kinesis&lt;/code&gt; appears. This is the policy you created previously.&lt;/li&gt; &lt;li&gt;Select the checkbox associated with the policy.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Next: Review&lt;/strong&gt; button on the right side.&lt;/li&gt; &lt;li&gt;Review the policy assignment and then click the &lt;strong&gt;Add permissions&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;The summary page for the user shows its Amazon Resource Name (ARN).&lt;/li&gt; &lt;li&gt;On the user's &lt;strong&gt;Permissions&lt;/strong&gt; tab, the &lt;code&gt;general-write-access-to-kinesis&lt;/code&gt; policy is shown applying to the user.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;At this point, you can create an application that uses the AWS SDK to write to a Kinesis stream as the user &lt;code&gt;my_kinesis_user&lt;/code&gt;. The program will need the access key ID and secret access key ID credentials you created earlier.&lt;/p&gt; &lt;h2&gt;Stream data to Amazon Kinesis using the AWS SDK&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; application we'll use to demonstrate the use of the AWS SDK is stored as a GitHub project named &lt;code&gt;kinesis-streamer&lt;/code&gt;. The application is intended to show you how to write to a Kinesis stream from outside of AWS. You can find the source code in &lt;a href="https://github.com/reselbob/kinesis-streamer"&gt;my GitHub repository&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Figure 7 shows a screenshot of three instances of &lt;code&gt;kinesis-streamer&lt;/code&gt; submitting data to a single Kinesis stream. This is a real-world example of the "many producer to single stream" pattern shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/streamer.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/streamer.png?itok=tNTwoPZz" width="1139" height="670" alt="Three produces are streaming JSON into Kinesis." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7. Three produces are streaming JSOn into Kinesis. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Each instance of &lt;code&gt;kinesis-streamer&lt;/code&gt; creates a number of &lt;code&gt;cron&lt;/code&gt; jobs that fire every second. Each &lt;code&gt;cron&lt;/code&gt; job sends a number of messages to a particular Kinesis stream. By default, the application creates ten &lt;code&gt;cron&lt;/code&gt; jobs, each of which submits ten messages to the Kinesis stream.&lt;/p&gt; &lt;p&gt;A programmer binds to an Amazon Kinesis stream by setting values to environment variables that allow the application to write to the defined Kinesis stream as a particular AWS user. The user is identified by its AWS access key ID and secret access key. Also, the target Kinesis stream is declared by an environment variable. In addition, the developer can set environment variables to override the default number of &lt;code&gt;cron&lt;/code&gt; jobs and messages to create. You can read the details of how to get the application up and running at its &lt;a href="https://github.com/reselbob/kinesis-streamer/blob/main/README.md"&gt;GitHub repository&lt;/a&gt;. The following settings illustrate how to configure the environment variables:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;AWS_ACCESS_KEY_ID="a^ACcEsS_KEY_tokEN!" AWS_SECRET_ACCESS_KEY="a^SecRET_accESS_k3y_TOken" AWS_KINESIS_STREAM_NAME=my-stream-kinesis CRON_JOBS_TO_GENERATE=50 MESSAGES_PER_CRON_JOB=20&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; Be advised that the values assigned to the environment variables &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; shown in the preceding snippet are only fictitious placeholder values.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;kinesis-streamer&lt;/code&gt; project uses the &lt;a href="https://aws.amazon.com/sdk-for-javascript/"&gt;AWS SDK for JavaScript&lt;/a&gt; to create a JavaScript client running under Node.js that writes to a Kinesis stream. There are many other ways to send data to Kinesis from both outside and inside AWS. You can use a technology such as AWS's &lt;a href="https://awslabs.github.io/amazon-kinesis-data-generator/web/help.html"&gt;Kinesis Data Generator (KDG)&lt;/a&gt; to send messages externally to an internal Kinesis stream. Or you can use an AWS Lambda function to send messages internally to Kinesis from within AWS. Of course, you can always create your own program using one of the many programming languages that are supported by the AWS SDK.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;kinesis-streamer&lt;/code&gt; project demonstrates such possibilities. So you might want to examine the &lt;a href="https://github.com/reselbob/kinesis-streamer"&gt;source code in the demonstration project&lt;/a&gt; to learn the details of working with Kinesis streams using the AWS SDK. The code is well documented to make it easier for developers to absorb the details.&lt;/p&gt; &lt;h2&gt;Get performance data using Kinesis stream reports&lt;/h2&gt; &lt;p&gt;Message activity in a Kinesis stream can be monitored using a number of graphical reports that AWS provides out of the box. From a developer's point of view, at the very least, you'll want to know whether the messages your application emits are actually making it into the Kinesis stream manager. These reports will tell you that information at a glance.&lt;/p&gt; &lt;p&gt;For example, Figure 8 shows reports for incoming data. These reports indicate that the stream received records. As simple as it sounds, this is very useful information.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/reporting.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/reporting.png?itok=sX83EaNY" width="931" height="510" alt="The Kinesis report for incoming data shows several statistics, such as the amount of messages and bytes received." loading="lazy" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8. The Kinesis report for incoming data shows several statistics, such as the amount of messages and bytes received. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;There are a number of other reports available for monitoring stream activity, such as &lt;strong&gt;latency&lt;/strong&gt;, &lt;strong&gt;errors&lt;/strong&gt;, and &lt;strong&gt;throughput reached&lt;/strong&gt;, to name a few&lt;strong&gt;.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Kinesis reporting allows developers to see the results of their application's streaming activity. This information is useful not only for smoke testing on the fly, but also for more extensive troubleshooting when trying to optimize the overall performance of a Kinesis stream.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;The information in this article shows you how to create a Kinesis data stream and how to send data to that stream. However, as mentioned earlier, simply sending data to a stream isn't enough. In order for the stream to be useful, there need to be consumers on the other end of the stream that are processing the incoming messages.&lt;/p&gt; &lt;p&gt;Writing consumers is a topic worthy of an article all to itself. Each consumer satisfies a specific use case. Not only do developers need to create programming logic for their producers and consumers, but they also need to decide which programming language best suits the need at hand. For example, Node.js JavaScript applications are operating system agnostic and a bit simpler to program in terms of language syntax, yet consumers written in Go run a lot faster. There's a lot to consider.&lt;/p&gt; &lt;p&gt;As mentioned at the beginning of this article, data streams add a new dimension to application development. Applications that use data streams make online services such as Netflix and Hulu possible. Yet technology does not stand still. New use cases are sure to emerge, and new streaming technologies will appear to meet many of the new challenges that older technologies can't address. As a result, developers who master the intricacies of working with data streams today are sure to enjoy a prosperous career in the future that's to come.&lt;/p&gt; &lt;h3&gt;Resources&lt;/h3&gt; &lt;p&gt;Check out the following resources to learn more:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/reselbob/kinesis-streamer"&gt;Demo source code&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/developer-sandbox/activities/connecting-to-your-managed-kafka-instance"&gt;Activity: Connecting to your Managed Kafka instance from the Developer Sandbox for Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://aws.amazon.com/kinesis/data-streams/"&gt;Amazon Kinesis Data Streams&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/09/create-data-stream-amazon-kinesis" title="Create a data stream with Amazon Kinesis"&gt;Create a data stream with Amazon Kinesis&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Bob Reselman</dc:creator><dc:date>2022-02-09T07:00:00Z</dc:date></entry><entry><title>Automate and deploy a JBoss EAP cluster with Ansible</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible" /><author><name>Romain Pelisse</name></author><id>edefe4d3-3bd3-407d-aaae-110b95ce03f3</id><updated>2022-02-08T07:00:00Z</updated><published>2022-02-08T07:00:00Z</published><summary type="html">&lt;p&gt;In my series introducing &lt;a href="https://developers.redhat.com/blog/2020/11/06/wildfly-server-configuration-with-ansible-collection-for-jcliff-part-1/"&gt;WildFly server configuration with Ansible collection for JCliff&lt;/a&gt;, I described how developers can use &lt;a href="https://www.ansible.com"&gt;Ansible&lt;/a&gt; to manage a standalone &lt;a href="https://developers.redhat.com/eap/download"&gt;Red Hat JBoss Enterprise Application Platform&lt;/a&gt; (JBoss EAP) instance. I've also written about using Ansible to &lt;a href="https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible"&gt;automate Apache Tomcat and Red Hat JBoss Web Server deployments&lt;/a&gt;. In this article, we'll go a bit deeper and use Ansible to deploy a fully operational &lt;a href="http://www.mastertheboss.com/jbossas/jboss-cluster/clustering-wildfly-application-server/"&gt;cluster of JBoss EAP instances&lt;/a&gt;. I'll show you how to automate the setup of each JBoss EAP instance and how to configure the network requirements—notably, fault tolerance and high availability—using features provided by the &lt;a href="https://www.wildfly.org"&gt;WildFly&lt;/a&gt; Ansible collection.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; This article assumes that you have prior knowledge of both Ansible and basic JBoss EAP/WildFly installation. Visit the &lt;a href="https://developers.redhat.com/courses/ansible/getting-started"&gt;Ansible courses page&lt;/a&gt; to learn the fundamentals of using Ansible.&lt;/p&gt; &lt;h2&gt;Use case: Deploying a JBoss EAP cluster with Ansible&lt;/h2&gt; &lt;p&gt;For this demonstration, we want to set up and run three JBoss EAP instances in a cluster. In this context, the application servers must communicate with each other to synchronize the content of the application's session. This configuration guarantees that, if one instance fails while processing a request, another one can pick up the work without any data loss.&lt;/p&gt; &lt;p&gt;We'll use a &lt;a href="https://en.wikipedia.org/wiki/Multicast"&gt;multicast&lt;/a&gt; to discover the members of the cluster and ensure that the cluster's formation is fully automated and dynamic.&lt;/p&gt; &lt;h2&gt;Step 1: Install Ansible collection for WildFly&lt;/h2&gt; &lt;p&gt;To follow this example, you need to install the Ansible collection that provides support for JBoss EAP. Named after JBoss EAP's upstream project, &lt;a href="http://github.com/ansible-middleware/wildfly/"&gt;Ansible collection for WildFly&lt;/a&gt; is part of the &lt;code&gt;middleware_automation&lt;/code&gt; collections and supplies a set of roles to simplify automation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-galaxy collection install middleware_automation.wildfly Process install dependency map Starting collection install process Installing 'middleware_automation.wildfly:0.0.2' to '/root/.ansible/collections/ansible_collections/middleware_automation/wildfly' Installing 'middleware_automation.redhat_csp_download:1.2.1' to '/root/.ansible/collections/ansible_collections/middleware_automation/redhat_csp_download&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note the following:&lt;/p&gt; &lt;p&gt;The content of Ansible collection for WildFly comes from the &lt;a href="https://galaxy.ansible.com/wildfly/jcliff"&gt;Ansible collection for JCliff&lt;/a&gt;, which is used to help deploy applications and fine-tune server configurations. To keep things simple, we won't use JCliff's features in this example. The part of the WildFly collection that we are using has been separated from the JCliff collection so that developers can use the WildFly features without having to install JCliff. Features of JCliff are not required for all use cases involving JBoss EAP.&lt;/p&gt; &lt;p&gt;Additionally, note that the &lt;code&gt;middleware_automation&lt;/code&gt; collections are provided through &lt;a href="https://galaxy.ansible.com"&gt;Ansible Galaxy&lt;/a&gt; and are not certified for &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;. Those certified collections are provided by &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/1.2/html/getting_started_with_red_hat_ansible_automation_hub/index"&gt;Red Hat Ansible Automation Hub&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Step 2: Set up the JBoss EAP cluster&lt;/h2&gt; &lt;p&gt;A typical JBoss EAP cluster has several machines, each operating a dedicated instance. In this case, for the simplicity of testing and reproducibility on a development system, we are going to use just one machine running several instances of JBoss EAP. The WildFly collection for Ansible makes it relatively easy to set up this architecture and provides all the required plumbing.&lt;/p&gt; &lt;p&gt;There are two parts to setting up the JBoss EAP cluster:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;&lt;strong&gt;Install JBoss EAP on the hosts.&lt;/strong&gt; This installation involves authenticating against the Red Hat Network (RHN), downloading the archive, and decompressing the archive in the appropriate directory (&lt;code&gt;JBOSS_HOME&lt;/code&gt;). These tasks are handled by the &lt;code&gt;wildfly_install&lt;/code&gt; role supplied by &lt;code&gt;wildfly&lt;/code&gt; collection.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Create the configuration files to run several instances of JBoss EAP.&lt;/strong&gt; Because we're running multiple instances on a single host, you also need to ensure that each instance has its own subdirectories and set of ports, so that the instances can coexist and communicate. Fortunately, this functionality is provided by a role within the Ansible collection called &lt;code&gt;wildfly_systemd&lt;/code&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;Note that if the variables &lt;code&gt;rhn_username&lt;/code&gt; and &lt;code&gt;rhn_password&lt;/code&gt; are defined, the collection automatically downloads the latest available version of JBoss EAP. If not, the &lt;code&gt;wildfly_install&lt;/code&gt; role fetches the latest upstream WildFly version. To avoid adding the credentials in our playbook, we incorporate them into a separate file named &lt;code&gt;rhn-creds.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- rhn_username: rhn_password: jboss_eap_rhn_id: &lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note:&lt;/strong&gt; We could use &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/vault.html"&gt;Ansible's vault&lt;/a&gt; feature to safely encrypt the credential values, but doing that is out of the scope of this article.&lt;/p&gt; &lt;h3&gt;Ansible playbook to install JBoss EAP&lt;/h3&gt; &lt;p&gt;Here is our Ansible playbook for installing and configuring JBoss EAP:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss EAP installation and configuration" hosts: "{{ hosts_group_name | default('localhost') }}" become: yes vars: wildfly_install_workdir: '/opt' wildfly_version: '7.4' install_name: jboss-eap wildfly_archive_filename: "{{ install_name }}-{{ wildfly_version }}.zip" wildfly_user: "{{ install_name }}" wildfly_config_base: standalone-ha.xml wildfly_home: "{{ wildfly_install_workdir }}/{{ install_name }}-{{ wildfly_version }}" jboss_eap_rhn_id: 99481 instance_http_ports: - 8180 - 8280 - 8380 app: name: 'info-1.1.war' url: 'https://drive.google.com/uc?export=download&amp;id=1w9ss5okctnjUvRAxhPEPyC7DmbUwmbhb' collections: - middleware_automation.redhat_csp_download - middleware_automation.wildfly roles: - redhat_csp_download - wildfly_install tasks: - name: "Set up for WildFly instance {{ item }}" include_role: name: wildfly_systemd vars: wildfly_config_base: 'standalone-ha.xml' wildfly_basedir_prefix: "/opt/{{ inventory_hostname }}" wildfly_config_name: "{{ install_name }}" wildfly_port_range_offset: -1 wildfly_instance_name: "{{ install_name }}" instance_id: "{{ item }}" service_systemd_env_file: "/etc/eap-{{ item }}.conf" service_systemd_conf_file: "/usr/lib/systemd/system/jboss-eap-{{ item }}.service" loop: "{{ range(0,3) | list }}" post_tasks: - set_fact: instance_http_ports: - 8180 - 8280 - 8380 - wait_for: port: "{{ item }}" loop: "{{ instance_http_ports }}" - name: "Checks that WildFly server is running and accessible" get_url: url: "http://localhost:{{ item }}/" dest: '/dev/null' loop: "{{ instance_http_ports }}" &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Run the playbook&lt;/h3&gt; &lt;p&gt;Now, let's run our Ansible playbook and check the resulting output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ansible-playbook -e @creds.yml jboss_eap.yml PLAY [WildFly installation and configuration] ****************************************************************** TASK [Gathering Facts] ***************************************************************************************** ok: [localhost] TASK [middleware_automation.redhat_csp_download.redhat_csp_download : assert] … TASK [Checks that WildFly server is running and accessible] **************************************************** ok: [localhost] =&gt; (item=8180) ok: [localhost] =&gt; (item=8280) ok: [localhost] =&gt; (item=8380) PLAY RECAP ***************************************************************************************************** localhost : ok=90 changed=0 unreachable=0 failed=0 skipped=14 rescued=0 ignored=0 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Although the playbook is quite short, it performs almost 100 tasks. First, it automatically installs the dependencies for &lt;code&gt;middleware_automation.redhat_csp_download&lt;/code&gt; by adding the associated role. Then, the &lt;code&gt;wildfly_install&lt;/code&gt; role uses the provided credentials to connect to RHN and download the &lt;code&gt;jboss-eap-7.4.zip&lt;/code&gt; file. Finally, once those steps have been completed successfully, the &lt;code&gt;wildfly_systemd&lt;/code&gt; role sets up three distinct services, each with its own set of ports and directory layout to store instance-specific data.&lt;/p&gt; &lt;p&gt;Note that the JBoss EAP installation is &lt;strong&gt;not duplicated&lt;/strong&gt;. All of the binaries live under the &lt;code&gt;/opt/jboss-eap-74&lt;/code&gt; directory. The separate directories simply store the runtime data for each instance.&lt;/p&gt; &lt;p&gt;On top of everything, we configured the instances to use the &lt;code&gt;standalone-ha.xml&lt;/code&gt; configuration as the baseline, so they are already set up for clustering.&lt;/p&gt; &lt;h2&gt;Step 3: Confirm the JBoss EAP instance and services are running&lt;/h2&gt; &lt;p&gt;The playbook confirms that each instance can be reached through its own HTTP port. We can also verify that the services are running by using the &lt;code&gt;systemctl&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl status jboss-eap-* ● jboss-eap-0.service - JBoss EAP (standalone mode) Loaded: loaded (/usr/lib/systemd/system/jboss-eap-0.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2021-12-23 12:31:41 UTC; 1min 55s ago Main PID: 1138 (standalone.sh) Tasks: 70 (limit: 1638) Memory: 532.3M CGroup: /system.slice/jboss-eap-0.service ├─1138 /bin/sh /opt/jboss-eap-7.4/bin/standalone.sh -c jboss-eap-0.xml -b 0.0.0.0 -Djboss.server.con&gt; └─1261 java -D[Standalone] -server -verbose:gc -Xloggc:/opt/localhost0/log/gc.log -XX:+PrintGCDetail&gt; Dec 23 12:31:44 7b38800644ee standalone.sh[1138]: 12:31:44,548 INFO [org.jboss.as.patching] (MSC service thread) Dec 23 12:31:44 7b38800644ee standalone.sh[1138]: 12:31:44,563 WARN [org.jboss.as.domain.management.security] &gt; Dec 23 12:31:44 7b38800644ee standalone.sh[1138]: 12:31 …&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 4: Deploy an application to the JBoss EAP cluster&lt;/h2&gt; &lt;p&gt;At this point, the three JBoss EAP instances are configured for clustering. However, no applications were deployed, so the cluster is not active (there is nothing to keep synchronized between all the instances).&lt;/p&gt; &lt;p&gt;Let's modify our Ansible playbook to deploy a simple application to all three JBoss EAP instances. To achieve this, we'll leverage another role provided by the &lt;code&gt;wildfly&lt;/code&gt; collection: &lt;code&gt;jboss_eap&lt;/code&gt;. This role includes a set of tasks generally focused on features specific to JBoss EAP.&lt;/p&gt; &lt;p&gt;In our case, we will use the &lt;code&gt;jboss_cli.yml&lt;/code&gt; task file, which encapsulates the running of JBoss command-line interface (CLI) queries:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;… - name: "Ensures webapp {{ app.name }} has been retrieved from {{ app.url }}" get_url: url: "{{ app.url }}" dest: "{{ wildfly_install_workdir }}/{{ app.name }}" - name: "Deploy webapp" include_role: name: jboss_eap tasks_from: jboss_cli.yml vars: jboss_home: "{{ wildfly_home }}" query: "'deploy --force {{ wildfly_install_workdir }}/{{ app.name }}'" jboss_cli_controller_port: "{{ item }}" loop: - 10090 - 10190 - 10290 …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now, we will once again execute our playbook so that the web application is deployed on all instances. Once the automation completes successfully, the deployment will trigger the formation of the cluster.&lt;/p&gt; &lt;h2&gt;Step 5: Verify the JBoss EAP cluster and application deployment&lt;/h2&gt; &lt;p&gt;You can verify the JBoss EAP cluster formation by looking at the log files of any of the three JBoss EAP instances:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;… 021-12-23 15:02:08,252 INFO [org.infinispan.CLUSTER] (thread-7,ejb,jboss-eap-0) ISPN000094: Received new cluster view for channel ejb: [jboss-eap-0|2] (3) [jboss-eap-0, jboss-eap-1, jboss-eap-2] …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To be thorough, you can also check that the application is properly deployed and accessible. To validate the application's operation, we can simply add a separate Ansible playbook called &lt;code&gt;validate.yml&lt;/code&gt;. We can then import the new playbook into our &lt;code&gt;playbook.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;post_tasks: - include_tasks: validate.yml loop: "{{ instance_http_ports }}"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;validate.yml&lt;/code&gt; file contains the following:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - assert: that: - item is defined - wait_for: port: "{{ item }}" - name: "Checks that WildFly server is running and accessible on port {{ item }}" get_url: url: "http://localhost:{{ item }}/" dest: '/dev/null' changed_when: False - include_tasks: info.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You might have noticed that we include another playbook, &lt;code&gt;info.yml&lt;/code&gt;, which is here:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - assert: that: - item is defined quiet: true - set_fact: result_file: "/tmp/info-{{ item }}.txt" - get_url: url: "http://localhost:{{ item }}/info/" dest: "{{ result_file }}" changed_when: False - slurp: src: "{{ result_file }}" register: info_res - debug: msg: "{{ info_res['content'] | b64decode }}&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To complete the exercise, we can run the validation playbook and see whether it confirms that our setup is fully functional:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;TASK [get_url] ******************************************************************************** changed: [localhost] TASK [slurp] ********************************************************************************** ok: [localhost] TASK [debug] ********************************************************************************** ok: [localhost] =&gt; { "msg": "Request received&lt;br/&gt;Requested URL:\t\t\thttp://localhost:8380/info/&lt;br/&gt;Runs on node:\t\t\tda953ac17443 [IP: 10.0.2.100 ]&lt;br/&gt;Requested by:\t\t\t127.0.0.1 [IP: 127.0.0.1, port: 40334 ]&lt;br/&gt;JBOSS_ID:\t\t\tnull&lt;br/&gt;" }&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Next steps with Ansible and JBoss EAP&lt;/h2&gt; &lt;p&gt;In this article, we've fully automated the setup and configuration of a three-instance cluster of JBoss EAP, along with an example web application as a workload. The playbook that we created for the automation is simple and straightforward. Most importantly, we were able to focus primarily on deploying the application. The WildFly collection for Ansible provided all the plumbing we needed to set up the JBoss EAP cluster.&lt;/p&gt; &lt;p&gt;You can find the source code for the example in the &lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo"&gt;WildFly cluster demo&lt;/a&gt; GitHub repository. For a more complex scenario, see the &lt;a href="https://github.com/ansible-middleware/flange-demo"&gt;Flange project demo&lt;/a&gt;, which adds to the JBoss EAP cluster an instance of Red Hat's &lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;single sign-on technology&lt;/a&gt;, using &lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt; as a cache and &lt;a href="https://www.redhat.com/en/resources/jboss-core-services-collection-datasheet"&gt;Red Hat Middleware Core Services Collection&lt;/a&gt; as a load balancer.&lt;/p&gt; &lt;p&gt;See these resources to learn more about using Ansible with JBoss EAP:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible"&gt;Automate Red Hat JBoss Web Server deployments with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2021/09/28/set-modcluster-red-hat-jboss-web-server-ansible"&gt;Set up mod_cluster for Red Hat JBoss Web Server with Ansible&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2020/11/06/wildfly-server-configuration-with-ansible-collection-for-jcliff-part-1"&gt;WildFly server configuration with Ansible collection for JCliff (three-part series)&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For more hands-on learning, in short, interactive courses, see the &lt;a href="https://developers.redhat.com/courses/ansible"&gt;Ansible courses page&lt;/a&gt;. Also, be sure to check out &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; for provisioning, deploying, and managing IT infrastructure across cloud, virtual, and physical environments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible" title="Automate and deploy a JBoss EAP cluster with Ansible"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2022-02-08T07:00:00Z</dc:date></entry></feed>
